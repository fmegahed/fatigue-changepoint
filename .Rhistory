?dfphase1::mphase1()
unlink('ChangepointAnalysisForFatigueData_cache', recursive = TRUE)
library(pacman)
pload(suncalc)
p_load(suncalc)
getSunlightPosition(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050)
getSunlightPosition(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050,
keep = c("sunrise","sunset","dawn","dusk"))
getSunlightPosition(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050,
keep = c("sunrise","sunsetEnd","dawn","dusk"))
getSunlightPosition(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050,
keep = "sunrise")
getSunlightTimes(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050)
getSunlightTimes(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050,
tz="ET")
getSunlightTimes(date = "2018-10-16",
lat = 39.51030,
lon = -84.742050)
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050)
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050,
tz= "ET")
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050,
tz= "EDT")
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050,
tz= "EST")
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050,
tz= "EDT")
getSunlightTimes(date = Sys.Date(),
lat = 39.51030,
lon = -84.742050,
tz= "America/New_York")
p_load(geotimes)
cities <- c('Sydney', 'SÃ£o Paulo', 'Milan', 'Chicago'
,'Mexico City', 'Mumbai', 'Moscow', 'Frankfurt'
,'Madrid', 'Warsaw', 'Johannesburg', 'Toronto'
,'Seoul', 'Istanbul', 'Kuala Lumpur', 'Jakarta'
,'Amsterdam', 'Brussels', 'Los Angeles')
latlong <- ggmap::geocode(cities)
View(latlong)
latlong <- UScitiesD
latlong <- map.cities()
df <- us.cities
df <- head(us.cities)
df <- head(maps::us.cities)
View(df)
df <- maps::us.cities
latlong_list <- {}
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long,
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
latlong_list <- {}
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
p_load(suncalc,dplyr,maps)
df <- maps::us.cities
latlong_list <- {}
a <- Sys.time()
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
b <- Sys.time()
b-a
latlong_list <- list(1005)
latlong_list <- vector("list", length = nrow(df))
a <- Sys.time()
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
b <- Sys.time()
b-a
latlong_list <- vector("list", length = nrow(df))
a <- Sys.time()
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
b <- Sys.time()
b-a
df <- maps::us.cities
latlong_list <- vector("list", length = nrow(df))
a <- Sys.time()
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise","dawn","dusk"))
}
b <- Sys.time()
b-a
latlong_list <- vector("list", length = nrow(df))
a <- Sys.time()
for (i in 1:nrow(df)) {
latlong_list[[i]] <-getSunlightTimes(date = Sys.Date(),
lat = df$lat[i],
lon = df$long[i],
tz= "America/New_York",
keep = c("sunset","sunrise"))
}
b <- Sys.time()
b-a
load("J:/My Drive/Miami/Research/2018/Amir - Changepoint Paper/Code/GitHub/fatigue-changepoint/Data/RGenerated/NormCusumData.RData")
a <- cusum_norm[[1]]
View(a)
a <- cusum_norm[[2]]
View(a)
a <- cusum_norm[[5]]
a <- cusum_norm[[15]]
is.na(a)
table(is.na(a$scaled.stride.len))
table(is.unique(a$scaled.stride.len))
table(unique(a$scaled.stride.len))
nrow(unique(a$scaled.stride.len))
length(unique(a$scaled.stride.len))
View(a)
?diss
?quantmod::diss
?Tsclust::diss
install.packages("TSclust")
library(TSclust)
?TSclust::diss
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
a = unlist(medianfilt_norm)
data("paired.tseries")
View(paired.tseries)
ts(medianfilt_norm[[1]][,2:4])
a <- ts(medianfilt_norm[[1]][,2:4])
a <- ts(medianfilt_norm[[1:15]][,2:4])
a <- ts(medianfilt_norm[[]][,2:4])
a <- lapply(medianfilt_norm function, function(x) x[,2:4])
a <- lapply(medianfilt_norm function, x[,2:4])
a <- lapply(medianfilt_norm function, function(x) x[,2:4])
a <- lapply(medianfilt_norm function, `[[`, 2:4])
a <- lapply(medianfilt_norm, function `[[`, 2:4])
a <- lapply(medianfilt_norm function, function(x) x[,2:4])
a <- lapply(medianfilt_norm, function(x) x[,2:4])
View(a)
library(dplyr)
a <- lapply(medianfilt_norm, function(x) x[,2:4]) %>% ts()
View(a)
a <- lapply(medianfilt_norm, function(x) x[,2:4]) %>% ts() %>%  unlist()
a <- lapply(medianfilt_norm, function(x) x[,2:4]) %>% matrix(nrow=2000, ncol=15*3) %>%  ts()
View(a)
View(a)
a <- sapply(medianfilt_norm, function(x) x[,2:4])
View(a)
a <- sapply(medianfilt_norm, function(x) x[,2:4]) %>% lapply(function(x) ts())
View(a)
a <- sapply(medianfilt_norm, function(x) x[,2:4]) %>% data.frame()
View(a)
deucl <- diss(a, "EUCL")
a <- sapply(medianfilt_norm, function(x) x[,2:4])
View(a)
deucl <- diss(sapply(a, `[[`, 1:3), "EUCL")
deucl <- diss(sapply(a, `[[`, 1), "EUCL")
?mts()
deucl <- diss(a, "EUCL")
hceucl <- hclust(deucl, "complete")
hceucl
plot(hclust)
?hclust
hceucl$labels
hceucl$order
plot(hceucl, hang=0.1, check = TRUE, axes = TRUE, ann = TRUE, ylab="Height")
View(a)
multivariate_data <- lapply(medianfilt_norm, function(x) x[,2:4])
View(multivariate_data)
a <- do.call(rbind, Map(data.frame, medianfilt_norm))
View(a)
a <- do.call(rbind, Map(data.frame, medianfilt_norm[[]][2:4]))
a <- do.call(rbind, Map(data.frame, medianfilt_norm[2:4]))
View(a)
a <- do.call(cbind, Map(data.frame, medianfilt_norm)
)
View(a)
deucl <- diss(a, "DTWARP")
knitr::opts_chunk$set(echo = TRUE)
deucl <- diss(a, "EUCL")
deucl <- diss(a, "EUCL")
plot(hceucl, hang=0.1, check = TRUE, axes = TRUE, ann = TRUE, ylab="Height")
deucl <- diss(multivariate_data, "DTWARP")
View(paired.tseries)
true_mts <- data.frame(matrix(a, nrow = 4, ncol = 15))
View(true_mts)
true_mts <- true_mts[-1,]
deucl <- diss(true_mts, "EUCL")
View(medianfilt_norm)
?hclust
distances <- diss(pca_subjects_scaled, "DTWARP")
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
pca_subjects_loadings_scaled <- list()
pca_subjects_scaled <- data.frame(matrix(nrow=2000, ncol=15))
pc_prop_var_explained_scaled <- vector()
for (i in 1:length(medianfilt_norm)) {
df <- medianfilt_norm[[i]]
pca_dataholder <-  prcomp(df[,2:4], center=TRUE, scale. = T) # Scale <- Standardize Vars
pca_subjects_loadings_scaled[[i]] <- pca_dataholder$rotation[,1] # Weights for First PC
# Understanding the % Variation Explained by the First PC
pc_std <- pca_dataholder$sdev
pc_var <- pc_std^2
pc_prop_var_explained_scaled[i] <- round(pc_var[1]/sum(pc_var),3)
#  Storing the Data for First PC rotated values
pca_subjects_scaled[,i] <- pca_dataholder$x[,1] # First PC
}
distances <- diss(pca_subjects_scaled, "DTWARP")
View(pca_subjects_scaled)
cat(paste0('<source> <p> Based on the above analysis, the % variation explained by the first principal component of the scaled median filtered data for each participant is equal to: ', paste(round(pc_prop_var_explained_scaled, 2), collapse = ", "), '. To understand how the underlying multivariate time series methods are similar, we applied a heirarchical clustering algorithm based on the distances obtained using the <i> Dynamic Time Warping Method </i>. A heatmap based on the distances obstained from the data of the fiften participants is shown in the figure below. </p> </source>'))
cat(" \n")
heatmap(distances)
heatmap(as.matrix(distances))
heatmap(as.matrix(distances))
a <- as.matrix(distances)
View(a)
colnames(a) <- paste0("P",seq(1,15,1))
View(a)
distances.matrix <- as.matrix(distances)
colnames(distances.matrix) <- paste0("P",seq(1,15,1))
rownames(distances.matrix) <- paste0("P",seq(1,15,1))
heatmap(distances.matrix)
heatmap(distances.matrix)
cat("\n")
cat("The resulting dendogram is shown in the figure below. \n")
clsuters <- hclust(distances)
str(clusters)
clusters <- hclust(distances)
str(clusters)
clusters <- hclust(distances, labels(paste0("P",seq(1,15,1))))
?hclust
clusters <- hclust(distances, labels= paste0("P",seq(1,15,1)))
clusters <- hclust(distances, labels= paste0("P",seq(1,15,1)))
clusters <- hclust(distances.matrix)
View(distances.matrix)
str(distances)
distances$Labels
distances["Labels"]
distances[["Labels"]]
distances[]["Labels"]
distances[[]]["Labels"]
distances[[5]]["Labels"]
distances[[5]]
dimnames(distances)
dimnames(distances) <- paste0("P",seq(1,15,1))
distances <- diss(pca_subjects_scaled, "DTWARP")
distances <- diss(pca_subjects_scaled, "DTWARP")
distances.matrix <- as.matrix(distances)
heatmap(distances.matrix)
heatmap(distances.matrix)
dimnames(distances) <- paste0("P",seq(1,15,1))
distances.matrix <- as.matrix(distances)
heatmap(distances.matrix)
cat("\n")
cat("The resulting dendogram is shown in the figure below. \n")
clusters <- hclust(distances)
plot(clusters, hang=0.1, check = TRUE,
axes = TRUE, ann = TRUE, ylab="Height",
main = "Cluster Dendogram based on the DTWARP distances")
plot(clusters, hang=0.1, check = TRUE,
axes = TRUE, ann = TRUE, ylab="Height",
main = "Cluster Dendogram based on the DTWARP distances")
cat("\n")
cat(paste0('<source> <p> Based on the above dendogram, one can see that the best choice for total number of clusters is equal to: 3. This number was then used to cut of the dendogram tree using the <i> cutree </i> function in R. We show the resulting cluster membership in the table below. </p> </source>'))
cat("\n")
clusterCut <- cutree(clusters, 3)
table(clusterCut)
clusterCut
print(clusterCut)
clusterCut <- cutree(clusters, 4)
print(clusterCut)
cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n',  print(clusterCut3), '. \n', 'On the other hand, when k=4, the following assignments were made: \n', print(clusterCut4), "\n"))
clusterCut3 <- cutree(clusters, 3)
clusterCut4 <- cutree(clusters, 4)
cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n',  print(clusterCut3), '. \n', 'On the other hand, when k=4, the following assignments were made: \n', print(clusterCut4), "\n"))
cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n'))
print(clusterCut3)
cat(paste0(' \n', 'On the other hand, when k=4, the following assignments were made: \n'))
print(clusterCut4)
cat("\n")
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman) # needs to be installed first
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
dplyr, stringr, tidyverse, utils, reshape2,
anomalize, forecast, MVN, fractal,
ecp, dfphase1,
MALDIquant, TSclust,
knitr)
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
pca_subjects_loadings_scaled <- list()
pca_subjects_scaled <- data.frame(matrix(nrow=2000, ncol=15))
pc_prop_var_explained_scaled <- vector()
for (i in 1:length(medianfilt_norm)) {
df <- medianfilt_norm[[i]]
pca_dataholder <-  prcomp(df[,2:4], center=TRUE, scale. = T) # Scale <- Standardize Vars
pca_subjects_loadings_scaled[[i]] <- pca_dataholder$rotation[,1] # Weights for First PC
# Understanding the % Variation Explained by the First PC
pc_std <- pca_dataholder$sdev
pc_var <- pc_std^2
pc_prop_var_explained_scaled[i] <- round(pc_var[1]/sum(pc_var),3)
#  Storing the Data for First PC rotated values
pca_subjects_scaled[,i] <- pca_dataholder$x[,1] # First PC
}
distances <- diss(pca_subjects_scaled, "DTWARP")
distances <- diss(pca_subjects_scaled, "DTWARP")
dimnames(distances) <- paste0("P",seq(1,15,1))
cat(paste0('<source> <p> Based on the above analysis, the % variation explained by the first principal component of the scaled median filtered data for each participant is equal to: ', paste(round(pc_prop_var_explained_scaled, 2), collapse = ", "), '. To understand how the underlying multivariate time series methods are similar, we applied a heirarchical clustering algorithm based on the distances obtained using the <i> Dynamic Time Warping Method </i>. The resulting dendogram is shown in the figure below. </p> </source>'))
cat(" \n")
clusters <- hclust(distances)
plot(clusters, hang=0.1, check = TRUE,
axes = TRUE, ann = TRUE, ylab="Height",
main = "Cluster Dendogram based on the DTWARP distances")
cat("\n")
cat(paste0('Based on the above dendogram, one can see that the best choice for total number of clusters is equal to: 3 or 4. This number was then used to cut of the dendogram tree using the  cutree function in R. We show the resulting cluster membership in the table below.'))
cat("\n")
clusters <- hclust(distances)
cat("\n")
cat(paste0('Based on the above dendogram, one can see that the best choice for total number of clusters is equal to: 3 or 4. This number was then used to cut of the dendogram tree using the  cutree function in R. We show the resulting cluster membership in the table below.'))
cat("\n")
clusterCut3 <- cutree(clusters, 3)
clusterCut4 <- cutree(clusters, 4)
cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n'))
a=
kable(clusterCut3)
a
clusterCut3
kable(clusterCut3) %>%  kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em") %>% cell_spec(x, color = factor(x, c("1", "2", "3"), c("red","blue","white")))
p_load(knitr, kableExtra)
kable(clusterCut3) %>%  kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em") %>% cell_spec(x, color = factor(x, c("1", "2", "3"), c("red","blue","white")))
kable(clusterCut3) %>%  kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em") %>% cell_spec(color = factor(c("1", "2", "3"), c("red","blue","white")))
kable(clusterCut3) %>%  kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em") %>% cell_spec(color =c("red","blue","white")))
kable(clusterCut3) %>%
kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em") %>%
cell_spec(color = c("red","blue","white"))
library(RColorBrewer)
?RColorBrewer
RColorBrewer::brewer.pal(3, Set3)
RColorBrewer::brewer.pal(3, "Set3")
clusterCut3
?mutate
clusterCut3 %>%   cell_spec(clusterCut3, "html",color = RColorBrewer::brewer.pal(4, "Set1"))
clusterCut3
clusterCut3[1]
names(clusterCut3)
factor(clusterCut3)
clusters3 %>% dplyr::mutate(
Participant = names(clusterCut3),
Cluster.Num = cell_spec(factor(clusterCut3), "html",color = RColorBrewer::brewer.pal(3, "Set1")))
clusters3 = dplyr::mutate(
Participant = names(clusterCut3),
Cluster.Num = cell_spec(factor(clusterCut3), "html",color = RColorBrewer::brewer.pal(3, "Set1")))
clusters3 = data.frame(
Participant = names(clusterCut3),
Cluster.Num = cell_spec(factor(clusterCut3), "html",color = RColorBrewer::brewer.pal(3, "Set1")))
# Printing the Results
load(file = "./Data/RGenerated/ScaledPcaClusters.RData")
cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n'))
kable(clusterCut3) %>%
kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em")
cat(paste0(' \n', 'On the other hand, when k=4, the following assignments were made: \n'))
kable(clusterCut4) %>%  kable_styling(bootstrap_options = "striped", full_width = F) %>% column_spec(1, "html", bold = T, width = "5em") %>%
column_spec(2, bold = F, width = "5em")
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman) # needs to be installed first
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
dplyr, stringr, tidyverse, utils, reshape2,
anomalize, forecast, MVN, fractal,
ecp, dfphase1,
MALDIquant, TSclust,
knitr, kableExtra)
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
dplyr, stringr, tidyverse, utils, reshape2,
anomalize, forecast, MVN, fractal,
ecp, dfphase1,
MALDIquant, TSclust,
knitr, kableExtra)
install.packages("TSclust")
library(TSclust)
install.packages("TSclust")
library(TSclust)
