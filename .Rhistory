df <- read_csv("J:/My Drive/Miami/Teaching/ISA 401/Exams/Exam II/Data/airline.csv")
df <- read_csv("J:/My Drive/Miami/Teaching/ISA 401/Exams/Exam II/Data/airline.csv")
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
View(df)
?subset
df <- subset(df,select = c(DayofMonth,Cancelled==0,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup))
summary(df$UniqueCarrier)
table(df$UniqueCarrier)
df <- subset(df,UniqueCarrier %in% c(AA,DL,OO,UA))
df <- subset(df,UniqueCarrier %in% c("AA","DL","OO","UA"))
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
df <- subset(df,select = c(DayofMonth,Cancelled==0,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup))
df <- subset(df,Cancelled==0,select = c(DayofMonth,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup))
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
df <- subset(df,select = c(DayofMonth,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup,
Cancelled))
table(df$UniqueCarrier)
df <- subset(df,UniqueCarrier %in% c("AA","DL"))
df <- subset(df,Cancelled==0)
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
df <- subset(df,select = c(DayofMonth,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup,
Cancelled,CarrierDelay,WeatherDelay,NASDelay,
SecurityDelay,LateAircraftDelay))
df <- subset(df,UniqueCarrier %in% c("AA","DL"))
df <- subset(df,Cancelled==0)
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
df <- subset(df,select = c(DayofMonth,DayOfWeek,FlightDate,UniqueCarrier,
TailNum,Origin,OriginCityName,OriginStateName,
Dest,DestCityName,DestStateName,DepDelay,DepDelayMinutes,
ArrDelay,ArrDelayMinutes,Distance,DistanceGroup,
Cancelled,CarrierDelay,WeatherDelay,NASDelay,
SecurityDelay,LateAircraftDelay))
table(df$UniqueCarrier)
df.final <- subset(df,UniqueCarrier %in% c("AA","DL","UA"))
which(table(df$UniqueCarrier)<20000)
small_airlines <- which(table(df$UniqueCarrier)<20000)
df.final <- subset(df,UniqueCarrier %in% small_airlines)
small_airlines
colnames(small_airlines)
row.names(small_airlines)
row.names(small_airlines)
names(small_airlines)
small_airlines <- names(which(table(df$UniqueCarrier)<20000))
df.final <- subset(df,UniqueCarrier %in% small_airlines)
small_airlines <- names(which(table(df$UniqueCarrier)<18000))
small_airlines <- names(which(table(df$UniqueCarrier)<15000))
small_airlines <- names(which(table(df$UniqueCarrier)<16000))
small_airlines <- names(which(table(df$UniqueCarrier)<17000))
small_airlines <- names(which(table(df$UniqueCarrier)<18000))
small_airlines <- names(which(table(df$UniqueCarrier)<19000))
small_airlines <- names(which(table(df$UniqueCarrier)<20000))
df.final <- subset(df,UniqueCarrier %in% small_airlines)
df.final <- subset(df.final,Cancelled==0)
small_airlines <- names(which(table(df$UniqueCarrier)<20000))
df.final <- subset(df,UniqueCarrier %in% small_airlines)
df.final <- subset(df.final,Cancelled==0)
df.final <- subset(df,UniqueCarrier %in% c("DL","AA"))
small_airlines <- names(which(table(df$UniqueCarrier)<20000))
df.final <- subset(df,UniqueCarrier %in% small_airlines)
df <- read_csv("J:\\My Drive\\Miami\\Teaching\\ISA 401\\Exams\\Exam II\\Data\\airline.csv")
table(df.final$OriginCityName)
View(df.final)
library(tidyr)
?replace_na
df.final2 <- replace_na(df.final,replace = 0)
df.final[1:93051,19:23] %>% replace_na(0)
write.csv(df.final,file = "exam2.csv",na=0)
write.csv(df.final,file = "exam2.csv",na="0")
load("J:/My Drive/Miami/CADS/EconProject/R/V1/DescriptionLookup.Rdata")
View(descrip_df)
View(descrip_df)
setwd("J:/My Drive/Miami/Code/R")
install.packages("WDI")
# Source: Paul Bayer, http://wandelweb.de/tools/Funnel-R.pdf
setFunnel <- function(x=0, y=0) {
funnel <<- data.frame(x=x, y=y)
}
source('J:/My Drive/Miami/Teaching/ISA 365 - Guest Lecture/funnel.R', echo=TRUE)
# Source: Paul Bayer, http://wandelweb.de/tools/Funnel-R.pdf
setFunnel <- function(x=0, y=0) {
funnel <<- data.frame(x=x, y=y)
}
# --------------------------------------------------
# throw one marble trough the funnel and mark the coordinate
# --------------------------------------------------
oneThrow <- function(s=1, col="black") {
x<-rnorm(s)+funnel$x
y<-rnorm(s)+funnel$y
points(x,y,col=col,pch=20)
return(data.frame(x=x, y=y))
}
# --------------------------------------------------
# draw a new funnelsheet
# --------------------------------------------------
newFunnel <- function(title="Demings Funnel Experiment") {
setFunnel()
plot(-10:10,-10:10,type="n",main=title,xlab="",ylab="",axes=F)
lines(c(0,0), c(-3,+3), type="l", col="blue")
lines(c(-3,+3), c(0,0), type="l", col="blue")
box()
}
throws <- function(strategy=1, no=50, col="black") {
last <- data.frame(x=0, y=0)
for ( i in 1:no ) {
if ( strategy == 1 ) oneThrow(col=col)
else if ( strategy == 2 ) {
setFunnel(funnel$x-last$x, funnel$y-last$y)
last <- oneThrow(col=col) }
else if ( strategy == 3 ) {
setFunnel(-last$x, -last$y)
last <- oneThrow(col=col) }
else if ( strategy == 4 ) {
setFunnel(last$x, last$y)
last <- oneThrow(col=col) }
else cat("strategy not implemented\n")
}
}
?seq
heuristic <- seq(5,1000,5)
heuristic
# Link for Hamza
temp <- read.csv("https://fred.stlouisfed.org/graph/fredgraph.csv?id=A253RC1Q027SBEA&fq=Quarterly")
View(temp)
# Link for Hamza
temp <- read.csv("https://fred.stlouisfed.org/graph/fredgraph.csv?id=A253RC1Q027SBEA&fq=Yearly")
# Link for Hamza
temp <- read.csv("https://fred.stlouisfed.org/graph/fredgraph.csv?id=A253RC1Q027SBEA")
# Link for Hamza
temp <- read.csv("https://fred.stlouisfed.org/graph/fredgraph.csv?id=leu0252881900a")
View(temp)
install.packages("RJSDMX")
library(RJSDMX)
library(pacman)
p_load(RJSDMX,rJava)
p_load(RJSDMX,rJava)
if (Sys.getenv("JAVA_HOME")!="") Sys.setenv(JAVA_HOME="")
p_load(RJSDMX,rJava)
library(fromJSON)
library(FromJSON)
p_load(fromJSON)
p_load(jsonlite)
trial <- fromJSON("http://wits.worldbank.org/API/V1/SDMX/V21/datasource/TRN/reporter/840/partner/000/product/020110/year/2000/datatype/reported")
p_load(jsonlite,curl)
trial <- fromJSON("http://wits.worldbank.org/API/V1/SDMX/V21/datasource/TRN/reporter/840/partner/000/product/020110/year/2000/datatype/reported")
p_load(jsonlite,curl,XML)
trial <- xmlParse("http://wits.worldbank.org/API/V1/SDMX/V21/datasource/TRN/reporter/840/partner/000/product/020110/year/2000/datatype/reported")
trial <- xmlParse("http://wits.worldbank.org/API/V1/SDMX/V21/datasource/TRN/reporter/840/partner/000/product/020110/year/2000/datatype/reported")
trial <- xmlParse("http://wits.worldbank.org/API/V1/wits/WITSApiService.svc/datasource/trn/dataavailability/")
trial <- xmlParse("http://wits.worldbank.org/API/V1/SDMX/V21/rest/dataflow/wbg_wits/")
# A Program to Assess Correlation with Kendall
require(MASS)
# Generating a 10*2 matrix with a correlation of 0.2
out <- mvrnorm(10, mu = c(0,0), Sigma = matrix(c(1,0.2,0.2,1), ncol = 2),
empirical = TRUE) # Generating a 10*2 matrix with a correlation of 0.2
cor(out, method="kendall", use="pairwise")
cor.test(out[,1],out[,1], method="kendall", use="pairwise")
cor.test(out[,1],out[,2], method="kendall", use="pairwise")
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val <- test$p.value
?rep
p.val <- rep(99,times=10000) # initilization
?seq
j <- seq(from=0.15,to=0.8,by=0.01)
# A Program to Assess Correlation with Kendall
rm(list=ls())
cat("\014")
# Required Libraries
require(MASS) # for multivariate normal
counter = 0
# Required Libraries
require(MASS) # for multivariate normal
# Initilizing variables
p.val <- rep(99,times=10000) # initilization
j <- seq(from=0.15,to=0.8,by=0.01)
counter = 0
for (k in j){
counter = counter + 1
for (i in 1:10000) {
out <- mvrnorm(10, mu = c(0,0), Sigma = matrix(c(1,0.2,0.2,1), ncol = 2),
empirical = TRUE) # Generating a 10*2 matrix with a correlation of 0.2
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val[i,counter] <- test$p.value
}
}
p.val <- data.frame(matrix(nrow =10000, ncol = length(j)) # df initilization
p.val <- data.frame(matrix(nrow =10000, ncol = length(j))) # df initilization
p.val <- data.frame(matrix(nrow =10000, ncol = length(j))) # df initilization
counter = 0
for (k in j){
counter = counter + 1
for (i in 1:10000) {
out <- mvrnorm(10, mu = c(0,0), Sigma = matrix(c(1,0.2,0.2,1), ncol = 2),
empirical = TRUE) # Generating a 10*2 matrix with a correlation of 0.2
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val[i,counter] <- test$p.value
}
}
p.val.99thperc <- apply(p.val,2,quantile(p.val,0.99))
p.val.99thperc <- apply(p.val,2,quantile(x,0.99))
p.val.99thperc <- apply(p.val,2,quantile, probs=0.99)
p.val.99thperc <- apply(p.val,2,quantile, probs=0.99)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.99))
View(p.val.99thperc)
View(p.val)
summary(p.val.99thperc)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.95))
summary(p.val.99thperc)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.5))
summary(p.val.99thperc)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.01))
summary(p.val.99thperc)
# Initilizing variables
j <- seq(from=0.1,to=0.8,by=0.02)
p.val <- data.frame(matrix(nrow =10000, ncol = length(j))) # df initilization
counter = 0
for (k in j){
counter = counter + 1
for (i in 1:10000) {
out <- mvrnorm(20, mu = c(0,0), Sigma = matrix(c(1,0.2,0.2,1), ncol = 2),
empirical = TRUE) # Generating a 20*2 matrix with a correlation of 0.2
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val[i,counter] <- test$p.value
}
}
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.01))
summary(p.val.99thperc)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.05))
summary(p.val.99thperc)
View(p.val.99thperc)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.95))
p.val.99thperc <- data.frame(apply(p.val,2,quantile, probs=0.5))
summary(p.val[,20])
summary(p.val[,30])
summary(p.val[,40])
summary(p.val[,36])
p.val.99thperc <- data.frame(apply(p.val,2,<0.05))
p.val.99thperc <- data.frame(apply(p.val,2,<, 0.05))
p.val.99thperc <- data.frame(apply(p.val,2,eval, <0.05))
p.val.99thperc <- data.frame(apply(p.val,2,quantile, prob=0.05))
summary(p.val.99thperc)
j <- seq(from=0.1,to=0.95,by=0.05)
p.val <- data.frame(matrix(nrow =10000, ncol = length(j))) # df initilization
counter = 0
for (k in j){
counter = counter + 1
for (i in 1:10000) {
out <- mvrnorm(50, mu = c(0,0), Sigma = matrix(c(1,0.2,0.2,1), ncol = 2),
empirical = TRUE) # Generating a 20*2 matrix with a correlation of 0.2
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val[i,counter] <- test$p.value
}
}
j <- seq(from=0.1,to=0.95,by=0.05)
p.val <- data.frame(matrix(nrow =10000, ncol = length(j))) # df initilization
counter = 0
for (k in j){
counter = counter + 1
for (i in 1:10000) {
out <- mvrnorm(10, mu = c(0,0),
Sigma = matrix(c(1,j[counter],j[counter],1), ncol = 2),
empirical = TRUE) # Generating a 20*2 matrix with a correlation of 0.2
test <- cor.test(out[,1],out[,2], method="kendall", use="pairwise")
p.val[i,counter] <- test$p.value
}
}
p.val.95thperc <- data.frame(apply(p.val,2,quantile, prob=0.05))
summary(p.val.95thperc)
?countif
sum(p.val.95thperc<0.05)
p.val.99thperc <- data.frame(apply(p.val,2,quantile, prob=0.01))
sum(p.val.99thperc<0.01)
rm(test)
rm(c(out,counter))
rm(out,counter)
colnames(p.val,prefix = "Corr.Coeff=") <- j
View(p.val)
colnames(p.val,prefix = "Corr.Coeff=") <- as.character(j)
?colnames
colnames(p.val, prefix = "CorrCoeff") <- as.character(j)
ans <- as.character(j)
lengh(ans)
length(ans)
colnames(p.val, prefix = "CorrCoeff") <- t(as.character(j))
colnames(p.val) <- paste("Corr.Coeff=",as.character(j))
View(p.val)
colnames(p.val) <- paste("Corr.Coeff=",as.character(j),sep = "")
p.val.95thperc <- data.frame(apply(p.val,2,quantile, prob=0.05))
p.val.99thperc <- data.frame(apply(p.val,2,quantile, prob=0.01))
View(p.val.95thperc)
p.05.coefficients <- which(sum(p.val.95thperc<0.05))
p.05.coefficients <- which(p.val.95thperc<0.05)
p.05.coefficients <- names(p.val.95thperc<0.05)
p.05.coefficients <- row.names(p.val.95thperc<0.05)
p.05.coefficients <- row.names(which(p.val.95thperc<0.05))
p.05.coefficients <- row.names(p.val.95thperc[p.05.coefficients])
p.05.coefficients <- which(p.val.95thperc<0.05)
p.val.95thperc[9]
p.05.coefficients <- which(p.val.95thperc<0.05)
p.05.coefficients <- row.names(p.val.95thperc[,p.05.coefficients])
p.05.coefficients <- row.names(p.val.95thperc[p.05.coefficients,])
p.05.coefficients <- row.names(p.val.95thperc[9,])
p.05.coefficients <- p.val.95thperc[9,]
p.05.coefficients <- col.names(p.val.95thperc[9,])
p.05.coefficients <- colnames(p.val.95thperc[9,])
p.05.coefficients <- rownames(p.val.95thperc[9,])
View(p.val.99thperc)
p.05.coefficients <- which(p.val.95thperc<0.05)
p.05.coefficients <- which(p.val.95thperc<0.05)
p.01.coefficients <- which(p.val.99thperc<0.01)
save(file="KendallTau", list = c(p.05.coefficients,p.01.coefficients,
p.val.99thperc,p.val.95thperc,
p.val))
save(file="KendallTau.RData", list = c(p.05.coefficients,p.01.coefficients,
p.val.99thperc,p.val.95thperc,
p.val))
save(list = c(p.05.coefficients,p.01.coefficients,
p.val.99thperc,p.val.95thperc,
p.val), file="KendallTau.RData")
?save
save(c(p.05.coefficients,p.01.coefficients,
p.val.99thperc,p.val.95thperc,
p.val), file="KendallTau.RData")
save(p.05.coefficients,p.01.coefficients,
p.val.99thperc,p.val.95thperc,
p.val, file="KendallTau.RData")
# Package Installation and Setting the Working Dir
#--------------------------------------------------
#install.packages("pacman") # install if needed
library(pacman)
p_load(osmdata)
library(lpSolve)
f.obj < c(0.75, 0.5, 0.15)
f.obj <- c(0.75, 0.5, 0.15)
f.con <- matrix(c(35,0.5,0.5,
60, 300, 10,
30, 20, 10), nrow=3,
byrow = TRUE)
f.dir <- c(">=", ">=", ">=")
f.rhs <- c(0.5, 15, 4)
LP_Solution <- lp("min", f.obj, f.con, f.dir, f.rhs, all.bin = F)
LP_Solution
LP_Solution$solution
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman) # needs to be installed first
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
dplyr, stringr, tidyverse, utils, reshape2,
anomalize, MVN, fractal)
?data.frame
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/serie1_telde.RData")
View(day_df)
View(serie1)
str(serie1)
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/serie1_orotava.RData")
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/serie1_telde.RData")
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/data1_el_hierro.RData")
View(data.tree)
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/serie1_orotava.RData")
load("C:/Users/megahefm/AppData/Local/Temp/Temp1_Notebook-112_Prediction_Classification-master.zip/Notebook-112_Prediction_Classification-master/serie1_telde.RData")
str(data.tree)
str(serie1)
knitr::opts_chunk$set(echo = TRUE)
num_subjects <- seq(1, 15)
subject_heights <- c(1.71, 1.77, 1.71, 1.59, 1.69,
1.63, 1.60, 1.71, 1.67, 1.78,
1.68, 1.55, 1.83, 1.81, 1.89)
# Initilizing a df for summary data on participants
summary_df <- data.frame(matrix(nrow = 15, ncol = 9))
colnames(summary_df) <- c("Subject.Num", "num.rows",
"num.cols", "mean.scaled.stride.len",
"sd.scaled.stride.len",
"mean.scaled.stride.height",
"sd.scaled.stride.height",
"mean.stride.duration",
"sd.stride.duartion")
for (i in 1:length(num_subjects)) {
# Reading the .mat files from GitHub
raw_data <- readMat(paste0("https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/Raw/Subject",num_subjects[i],".mat?raw=true"))
# Compute the number of cells, and rows in each structered matrix
raw_data_size <- lengths(raw_data) # num of cells
num_rows <- raw_data_size / 17 # all data had 17 cols
# Initilizing the six lists needed for storing the data (we keep track of the top 3 for error checking)
time_in_sec <- vector("list", length = num_rows)
position_x <- vector("list", length = num_rows)
position_y <- vector("list", length = num_rows)
stride_time <- vector("list", length = num_rows)
stride_length <- vector("list", length = num_rows)
stride_height <- vector("list", length = num_rows)
stride_duration <- vector("list", length = num_rows)
# Following for loop is needed since R reads the structured array as a nested list. The list containing the data is called "M.i.k" and it transforms/reads the original array --> rowise. This means that our first three features (with the same timestamp) are always seperated with a distance equal to the total number of rows
for (j in 1:num_rows) {
position_x[[j]] <- raw_data[["M.i.k"]][[j]]
position_y[[j]] <- raw_data[["M.i.k"]][[num_rows + j]]
stride_time[[j]] <- raw_data[["M.i.k"]][[2 * num_rows + j]]
dataholder <- raw_data[["M.i.k"]][[16 * num_rows + j]] # data holder for time
# Computing the three needed kinematic features
stride_length[[j]] <-
range(position_x[[j]])[2] - range(position_x[[j]])[1]
stride_height[[j]] <-
range(position_y[[j]])[2] - range(position_y[[j]])[1]
stride_duration[[j]] <-
range(stride_time[[j]])[2] - range(stride_time[[j]])[1]
time_in_sec[[j]] <- lapply(dataholder, mean)# using mean time of stride as a time stamp
}
# Scaling and creating one data frame per subject
assign(paste0("subject", i, "_features"),
data.frame(time.from.start = unlist(time_in_sec),
scaled.stride.len = unlist(stride_length)/subject_heights[i],
scaled.stride.height = unlist(stride_height) / subject_heights[i],
stride.duration = unlist(stride_duration)
)
)
# Creating a Summary Data Frame
df_name <- paste0("subject", i, "_features")
summary_df[i, 1] <- paste0("subject", i)
summary_df[i, 2] <- get(df_name) %>% nrow()
summary_df[i, 3] <- get(df_name) %>% ncol()
summary_df[i, 4] <- get(df_name)[, 1] %>% mean() %>% round(digits = 4)
summary_df[i, 5] <- get(df_name)[, 1] %>% sd() %>% round(digits = 4)
summary_df[i, 6] <- get(df_name)[, 2] %>% mean() %>% round(digits = 4)
summary_df[i, 7] <- get(df_name)[, 2] %>% sd() %>% round(digits = 4)
summary_df[i, 8] <- get(df_name)[, 3] %>% mean() %>% round(digits = 4)
summary_df[i, 9] <- get(df_name)[, 3] %>% sd() %>% round(digits = 4)
}
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman) # needs to be installed first
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
dplyr, stringr, tidyverse, utils, reshape2,
anomalize, forecast, MVN, fractal)
setwd("J:/My Drive/Miami/Research/2018/Amir - Changepoint Paper/Code/GitHub/fatigue-changepoint")
num_subjects <- seq(1, 15)
subject_heights <- c(1.71, 1.77, 1.71, 1.59, 1.69,
1.63, 1.60, 1.71, 1.67, 1.78,
1.68, 1.55, 1.83, 1.81, 1.89)
# Initilizing a df for summary data on participants
summary_df <- data.frame(matrix(nrow = 15, ncol = 9))
colnames(summary_df) <- c("Subject.Num", "num.rows",
"num.cols", "mean.scaled.stride.len",
"sd.scaled.stride.len",
"mean.scaled.stride.height",
"sd.scaled.stride.height",
"mean.stride.duration",
"sd.stride.duartion")
for (i in 1:length(num_subjects)) {
# Reading the .mat files from GitHub
raw_data <- readMat(paste0("https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/Raw/Subject",num_subjects[i],".mat?raw=true"))
# Compute the number of cells, and rows in each structered matrix
raw_data_size <- lengths(raw_data) # num of cells
num_rows <- raw_data_size / 17 # all data had 17 cols
# Initilizing the six lists needed for storing the data (we keep track of the top 3 for error checking)
time_in_sec <- vector("list", length = num_rows)
position_x <- vector("list", length = num_rows)
position_y <- vector("list", length = num_rows)
stride_time <- vector("list", length = num_rows)
stride_length <- vector("list", length = num_rows)
stride_height <- vector("list", length = num_rows)
stride_duration <- vector("list", length = num_rows)
# Following for loop is needed since R reads the structured array as a nested list. The list containing the data is called "M.i.k" and it transforms/reads the original array --> rowise. This means that our first three features (with the same timestamp) are always seperated with a distance equal to the total number of rows
for (j in 1:num_rows) {
position_x[[j]] <- raw_data[["M.i.k"]][[j]]
position_y[[j]] <- raw_data[["M.i.k"]][[num_rows + j]]
stride_time[[j]] <- raw_data[["M.i.k"]][[2 * num_rows + j]]
dataholder <- raw_data[["M.i.k"]][[16 * num_rows + j]] # data holder for time
# Computing the three needed kinematic features
stride_length[[j]] <-
range(position_x[[j]])[2] - range(position_x[[j]])[1]
stride_height[[j]] <-
range(position_y[[j]])[2] - range(position_y[[j]])[1]
stride_duration[[j]] <-
range(stride_time[[j]])[2] - range(stride_time[[j]])[1]
time_in_sec[[j]] <- lapply(dataholder, mean)# using mean time of stride as a time stamp
}
# Scaling and creating one data frame per subject
assign(paste0("subject", i, "_features"),
data.frame(time.from.start = unlist(time_in_sec),
scaled.stride.len = unlist(stride_length)/subject_heights[i],
scaled.stride.height = unlist(stride_height) / subject_heights[i],
stride.duration = unlist(stride_duration)
)
)
# Creating a Summary Data Frame
df_name <- paste0("subject", i, "_features")
summary_df[i, 1] <- paste0("subject", i)
summary_df[i, 2] <- get(df_name) %>% nrow()
summary_df[i, 3] <- get(df_name) %>% ncol()
summary_df[i, 4] <- get(df_name)[, 1] %>% mean() %>% round(digits = 4)
summary_df[i, 5] <- get(df_name)[, 1] %>% sd() %>% round(digits = 4)
summary_df[i, 6] <- get(df_name)[, 2] %>% mean() %>% round(digits = 4)
summary_df[i, 7] <- get(df_name)[, 2] %>% sd() %>% round(digits = 4)
summary_df[i, 8] <- get(df_name)[, 3] %>% mean() %>% round(digits = 4)
summary_df[i, 9] <- get(df_name)[, 3] %>% sd() %>% round(digits = 4)
}
unlink('ChangepointAnalysisForFatigueData_cache', recursive = TRUE)
load(file = "./Data/RGenerated/FeatureGeneration.RData")
