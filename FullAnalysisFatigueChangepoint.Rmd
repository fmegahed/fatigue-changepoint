---
title: "Changepoint Analysis For Fatigue Data"
author:
  - Amir Baghdadi^[Department of Mechanical and Aerospace Engineering, University at Buffalo. ]
  - Lora A. Cavuoto^[Department of Industrial and Systems Engineering, University at Buffalo. ]
  - Allison Jones-Farmer^[Farmer School of Business, Miami University. ]
  - Steve Rigdon^[College for Public Health and Social Justice, Saint Louis University]
  - Fadel M. Megahed^[Farmer School of Business, Miami University. Corresponding author.    [fmegahed@miamioh.edu](mailto:fmegahed@miamioh.edu).]

date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: hide
---

  

****

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this paper, we attempt to examine whether the use of change-point analysis techniques is appropriate for detecting fatigue based on data captured from wearable sensors. As such, we perform a **secondary data analysis** to the data generated in: [Baghdadi et al., 2018](https://www.tandfonline.com/doi/full/10.1080/00140139.2018.1442936). The reader should note that their raw data was preprocessed using:  

  (A) *Kalman Filter:* Used to process the raw data from sensors to: (i) estimate the spatial orientation of the body with respect to the global reference frame, and (ii) to estimate the kinematics of motion.  
  
  (B) *Segmentation:* The motion segments where then segmented using an algorithm that assumes the existence of two peaks in the translational acceleration of the gait cycle. This assumption was justified based on the results of [Tongen and Wunderlich, 2010](https://www.cambridge.org/core/books/mathematics-and-sports/biomechanics-of-running-and-walking/0DC334F72A56527FC4517B297923E9D3) as well as the authors' preliminary analyses.  
  
The reader can **show** any code chunk by clicking on the *code* button. We chose to make the default for the code hidden since we: (a) wanted to improve the readability of this document; and (b) assumed that the readers will not be interested in reading every code chunk.

****

# Loading Data & Generating Features 

The snippet below documents the list of **R** libraries that were used in this research. For convenience, we used the pacman package since it allows for installing/loading the needed libraries in one step.

```{r, load_libraries, message=FALSE, cache=TRUE, error=FALSE, warning=FALSE}
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
library(pacman) # needs to be installed first
p_load(R.matlab, plotly, extrafont, grDevices, gridExtra,
       dplyr, stringr, tidyverse, utils, reshape2,
       anomalize, forecast, MVN, fractal,
       ecp, dfphase1, 
       MALDIquant, TSclust,
       knitr, kableExtra)  
```

In the snippet below, we extract the 15 ".mat" files in the [GitHub repository](https://github.com/fmegahed/fatigue-changepoint/tree/master/Data/Raw) (where we loaded the data to allow for the reproduction of our work). Note that these files were originally produced in [Baghdadi et al., 2018](https://www.tandfonline.com/doi/abs/10.1080/00140139.2018.1442936?). Then, we perform several transformation steps: (a) extracting the data for the first three columns in the matlab arrays; and (b) computing three kinematic features from the data corresponding to these columns. Due to the differences between Matlab and R, this requires two nested *for* loops. The outer loop increments over the number of subjects, while the inner loop increments based on the different number of rows of data for each subject. Please see the comments within the code chunk for more details. 

```{r load_data, cache=TRUE}
num_subjects <- seq(1, 15)
subject_heights <- c(1.71, 1.77, 1.71, 1.59, 1.69,
                     1.63, 1.60, 1.71, 1.67, 1.78,
                     1.68, 1.55, 1.83, 1.81, 1.89)

# Initilizing a df for summary data on participants
summary_df <- data.frame(matrix(nrow = 15, ncol = 9))
colnames(summary_df) <- c("Subject.Num", "num.rows",
                          "num.cols", "mean.scaled.stride.len",
                          "sd.scaled.stride.len",
                          "mean.scaled.stride.height",
                          "sd.scaled.stride.height",
                          "mean.stride.duration",
                          "sd.stride.duartion")

for (i in 1:length(num_subjects)) {
  # Reading the .mat files from GitHub
  raw_data <- readMat(paste0("https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/Raw/Subject",num_subjects[i],".mat?raw=true"))
  # Compute the number of cells, and rows in each structered matrix
  raw_data_size <- lengths(raw_data) # num of cells
  num_rows <- raw_data_size / 17 # all data had 17 cols
  # Initilizing the six lists needed for storing the data (we keep track of the top 3 for error checking)
  time_in_sec <- vector("list", length = num_rows)
  position_x <- vector("list", length = num_rows)
  position_y <- vector("list", length = num_rows)
  stride_time <- vector("list", length = num_rows)
  stride_length <- vector("list", length = num_rows)
  stride_height <- vector("list", length = num_rows)
  stride_duration <- vector("list", length = num_rows)
  
  # Following for loop is needed since R reads the structured array as a nested list. The list containing the data is called "M.i.k" and it transforms/reads the original array --> rowise. This means that our first three features (with the same timestamp) are always seperated with a distance equal to the total number of rows
  for (j in 1:num_rows) {
    position_x[[j]] <- raw_data[["M.i.k"]][[j]]
    position_y[[j]] <- raw_data[["M.i.k"]][[num_rows + j]]
    stride_time[[j]] <- raw_data[["M.i.k"]][[2 * num_rows + j]]
    dataholder <- raw_data[["M.i.k"]][[16 * num_rows + j]] # data holder for time
    # Computing the three needed kinematic features
    stride_length[[j]] <-
      range(position_x[[j]])[2] - range(position_x[[j]])[1]
    stride_height[[j]] <-
      range(position_y[[j]])[2] - range(position_y[[j]])[1]
    stride_duration[[j]] <-
      range(stride_time[[j]])[2] - range(stride_time[[j]])[1]
    time_in_sec[[j]] <- lapply(dataholder, mean)# using mean time of stride as a time stamp
  }
  
  # Scaling and creating one data frame per subject
  assign(paste0("subject", i, "_features"), 
         data.frame(time.from.start = unlist(time_in_sec), 
                    scaled.stride.len = unlist(stride_length)/subject_heights[i], 
                    scaled.stride.height = unlist(stride_height) / subject_heights[i], 
                    stride.duration = unlist(stride_duration)
                    )
         )
  
  # Creating a Summary Data Frame
  df_name <- paste0("subject", i, "_features")
  summary_df[i, 1] <- paste0("subject", i)
  summary_df[i, 2] <- get(df_name) %>% nrow()
  summary_df[i, 3] <- get(df_name) %>% ncol()
  summary_df[i, 4] <- get(df_name)[, 2] %>% mean() %>% round(digits = 4)
  summary_df[i, 5] <- get(df_name)[, 2] %>% sd() %>% round(digits = 4)
  summary_df[i, 6] <- get(df_name)[, 3] %>% mean() %>% round(digits = 4)
  summary_df[i, 7] <- get(df_name)[, 3] %>% sd() %>% round(digits = 4)
  summary_df[i, 8] <- get(df_name)[, 4] %>% mean() %>% round(digits = 4)
  summary_df[i, 9] <- get(df_name)[, 4] %>% sd() %>% round(digits = 4)
}
# Printing the top six rows of Subject 4's data as an example
head(subject4_features) %>% round(digits = 3)

# A Summary of the features for all 15 participants
summary_df
rm(raw_data, raw_data_size, i, j, num_rows, 
   dataholder, num_subjects)
save.image(file = "./Data/RGenerated/FeatureGeneration.RData")
```

Based on the analysis above, there are **three observations** to be made. *First*, we scaled the stride length and height based on the subject's height. This in essence allows us to capture the steps as a percentage of the person's height. This reduces the between subject variablity in the data and is supported by the seminal work of [Oberg et al., 1993](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.473.947&rep=rep1&type=pdf). *Second*, we showed the first six rows from Subject 4 to provide the reader with some insights into the sampling frequeny (after the preprocessing done in [Baghdadi et al. 2018](https://www.tandfonline.com/doi/abs/10.1080/00140139.2018.1442936)). Note that the kinematic features are computed from the sensor channels provided in their paper. *Third*, we saved the generated data into an R.data file, which can be accessed by clicking on:  [FeatureGeneration.RData](https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/FeatureGeneration.RData). We hope that this saved data allows other researchers to reproduce and/or build on our work.

****

# Detecting and Removing Outliers
In this task, we examined several approaches for *outlier detection*. We originally hypothesized that these outliers constitue faulty sensor readings (since they are not sustained), and thus, we can remove this data prior to any further analysis. **However, based on a detailed investigation of the data and the application of several outlier detection methods (univariate and multivariate), we have learned that the data is non-normal, highly autocorrelated and non-stationary. Thus, these approaches are not suitable for removing the outliers from our data.** Thus, we then investiaged the utilization of "filtering" based methods in Section 3 to impute/correct/smoothen the data. 

For the sake of reproducibility and completeness, we provide the results from our implementation of several outlier detection methods in the subsections below. *Note that these methods will not be incorporated in our final analysis.* They are presented here only to document how we reached our final model/approach. 

## Plotting the Data {.tabset .tabset-fade}
First, we use a standard line plot to depict the data for each feature by person. For the sake of facilitating the visualization process, we: (a) panel the plot such that the left, center and right panels correspond to the scaled stride length, scaled stride height and step duration, respectively; and (b) we save the results of each participant (P) in a different tab.
```{r LineGraph, cache=TRUE, fig.align='center', results='asis', out.width="65%"}
# We are reading the data locally (you can download the file from GitHub)
load(file = "./Data/RGenerated/FeatureGeneration.RData")
for (i in 1:15) {
  df_transformed <- melt(get(paste0("subject",i,"_features")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         )
  assign(paste0("p",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start,y=value,group=variable,color=variable,
                    shape=variable)) + 
           geom_line() + theme_bw() + 
           #ylim(0,2) +
           ggtitle(paste0("Participant ",i,": Pre-filtered Data")) + 
           theme(legend.position="none") + 
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  cat('###',paste0("P", i), "{-}",'\n')
  print(get(paste0("p",i)))
  cat('\n \n')
  
  stat_printdf <- get(paste0("subject",i,"_features"))
  
 cat(paste0('<source> <p> Based on the <b>raw data</b>, Participant ', i,
             ' has an average scaled stride length of ',
             round(mean(stat_printdf[,2]),2),
             ', an average scaled stride height of ',
             round(mean(stat_printdf[,3]),2),
             ' and an average stride duration of ', round(mean(stat_printdf[,4]),2),
             ' seconds. Based on their height of ', subject_heights[i],
             ' meters, their average stride length and stride height are', ' ',
             round(mean(stat_printdf[,2])*subject_heights[i],2),
             ' and ', ' ', round(mean(stat_printdf[,3])*subject_heights[i],2),
             ', respectively.', '</p> </source>'
             )
      )
cat(' \n \n')
}

```

## Implementation of Several Outlier Detection Methods

### Boxplots for Outlier Detection  {.tabset .tabset-fade}
One of the most commonly used approaches for univariate outliers are defined to be the observations that lie outside $\small 1.5 * IQR$, where the IQR is the inter quartile range. This can be easily implemented using in base **R** using the [boxplot.stats function](https://www.rdocumentation.org/packages/grDevices/versions/3.5.1/topics/boxplot.stats). Below, we first provide the output and data plots for each person.

```{r BoxOutlierDetection, cache=TRUE, fig.align='center', results='asis', out.width='65%'}
# We are reading the data locally (you can download the file from GitHub)
load(file = "./Data/RGenerated/FeatureGeneration.RData")

outliers_bp <- list() # initilizing a list to store outliers per participant
for (i in 1:15) {
  df <- get(paste0("subject",i,"_features")) # Getting Data from Sec.
  # Obtaining the Outliers for each of the three variables
  out_vals_sl<- boxplot.stats(df[,2])$out
  out_rows_sl <- which(df[,2] %in% out_vals_sl)
  out_vals_sh<- boxplot.stats(df[,3])$out
  out_rows_sh <- which(df[,3] %in% out_vals_sh)
  out_vals_sd<- boxplot.stats(df[,4])$out
  out_rows_sd <- which(df[,4] %in% out_vals_sd)
  # Generating a union set of all obs. that have outliers
  # True: if any of the 3 vars for that obs. is an outlier
  outliers_total <- unique(c(out_rows_sl, out_rows_sh,
                             out_rows_sd))
  outliers_bp[[i]] <- outliers_total # saving it to list indexed by participant number
  
  # Remove the observations corresponding to the outliers
  assign(paste0("subject",i,"_bp"), 
         df[-outliers_total,])  
  
  # Preparing the data for the Line Graph
  df_transformed <- melt(get(paste0("subject",i,"_bp")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
  
  assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           #ylim(0,2) +
           ggtitle("Outliers removed via the standard boxplot method \n (any point outside of 1.5*IQR was removed)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = 'free_y')
         )
  cat('####',paste0("P", i), "{-}",'\n')
  
    
    # Printing the % of outliers removed
    cat("\n")
    num_rows <- nrow(get(paste0("subject",i,"_features")))
    num_outliers <- length(outliers_total)
    percent_data <- round(100*num_outliers/num_rows,2)
    cat("<b>",paste0("% of removed Observations for Partcipant ",i,
                 ": ", percent_data,"%."), "</b>",
        "This removes any value outside 1.5 * IQR for any of the 3 features. From the code, it should be clear that we are implementing this columnwise, starting with stride length and ending with stride duration.")
    cat('\n \n')
  # Plotting the data without the outliers
  print(get(paste0("g",i)))
  cat('\n \n')
  
  stat_printdf <- get(paste0("subject",i,"_bp"))
  
 cat(paste0('<source> <p> After applying the <b>boxplot outlier detection methodology</b>, Participant ', i,
             ' has an average scaled stride length of ',
             round(mean(stat_printdf[,2]),2),
             ', an average scaled stride height of ',
             round(mean(stat_printdf[,3]),2),
             ' and an average stride duration of ', round(mean(stat_printdf[,4]),2),
             ' seconds. Based on their height of ', subject_heights[i],
             ' meters, their average stride length and stride height are', ' ',
             round(mean(stat_printdf[,2])*subject_heights[i],2),
             ' and ', ' ', round(mean(stat_printdf[,3])*subject_heights[i],2),
             ', respectively.', '</p> </source>'
             )
      )
  
cat('\n')
  

cat('<source> <p> Note that we could have tried making the value of the coefficient multiplied by the IQR coefficient larger (e.g., both the <i>tsoutlier() function</i> in the <a href="https://cran.r-project.org/web/packages/forecast/forecast.pdf">forecast package</a> and the <i> iqr() function </i> in the <a href="https://cran.r-project.org/web/packages/anomalize/anomalize.pdf">anomalize package</a> use a value of 3 for the coefficient); however, we did not explore this option since this: (a) would have possibly required a large amount of trial-and-error, and (b) the approach would be difficult to justify. </p> </source>')

cat(' \n \n')
}
# Saving a list of cleaned 
save(subject1_bp, subject2_bp, subject3_bp, subject4_bp,
     subject5_bp, subject6_bp, subject7_bp, subject8_bp,
     subject9_bp, subject10_bp, subject11_bp, subject12_bp,
     subject13_bp, subject14_bp, subject15_bp, outliers_bp,
     file = "./Data/RGenerated/OutliersRemovedBoxplot.RData")
```

### Mahalanobis Dist. based Method {.tabset .tabset-fade}
```{r Mahalanobis, cache=TRUE, fig.align='center', results= 'asis', out.width= "65%"}
# We are reading the data locally (you can download the file from GitHub)
load(file = "./Data/RGenerated/FeatureGeneration.RData")
mal_results <- {}
for (i in 1:15) {
  df <- get(paste0("subject",i,"_features"))
  df <- df[,2:4]#only taking the four numeric fields
  cat('####',paste0("P", i), "{-}",' \n')
  cat('The output from the <source> <i> MVN package </i> </source> is shown below. Note that we depict the multivariate normal distribution for the three features of each distribution below. </source> <b> Essentially, the goal here is two-fold. First, we would like to examine if the joint distribution of the three kinematic features can be modeled using the multivariate normal distribution. Second, if the first hypothesis is true, then we can use the Mahalanobis Distance to identify outliers in each observation. </b> </source>')

  plot.new()
  mvn_test <- mvn(df, mvnTest = "dh",desc = TRUE,
                univariateTest = "Lillie",
                multivariateOutlierMethod = "adj",
                showOutliers = TRUE) #adjusted Mahl Distance 
 cat(' \n')
  
  cat(paste0('In addition to the plot, we provide the statistics of the Doornik-Hansen test for Participant ',i,' below. \n \n'))
 mvn_Normality <- mvn_test$multivariateNormality
 rownames(mvn_Normality) <- paste('Participant',i)
 tab <- xtable::xtable(mvn_test$multivariateNormality,
                       align = c("c","c","c","c","c","c"))
 print(tab, type= 'html',
       html.table.attributes = 'align="center", rules="rows", 
                                width=50%, frame="below"')
  cat('<source> <p> \n </p> </source>')
  
  cat(paste0('Based on the MVN test and the plot above, it is clear that the data is not normally distributed. Thus, an outlier detection/removal algorithm based on a MVN distribution will be inappropriate. This observation is true to not only Participant ', i, ' but to also all the other participants. Thus, in the following section we will investigate method(s) based on the signal processing literature detection literature.'))
  
  cat(' \n \n')
  
}

```

# Filtering Approaches and Data Preprocessing Approaches
An alternative approach to detecting and then removing outliers is to use smoothing techniques to "correct" the data. Based on the results shown in the previous sections, the fundamental hypotheses behind this approach are: 
  
  (A) The sensors' data seem to be non-stationary, autocorrelated and somewhat complex (from a statistical perspective). More importantly, there are spikes in the data that cannot be explained by the kinematic model. These spikes produce values that are unrealistic (e.g., a scaled stride length > 2 is not possible since that means that the stride of the person is two times larger than their height). While this insight is useful, there is no hard limit that we can enforce since the literature reports mean values across the population instead of attempting to provide a physical threshold on what is possible.  
  
  (B) The outliers tended to appear in pairs of two; indicating a possible issue in the segmentation of the data. Note that we label this data as outliers since it is not sustained (i.e., it is unlikely to say that a person is fatigued for only two stride).  
  
  (C) Smoothing (i.e. a low pass filter on the data) will allow us to impute values for every stride (after an initial size window). This will allow us to preserve the successive nature of the strides, "fix" the outliers based on neighboring strides, and not throw away any observation.

## The Median Filtering Approach from the Signal Processing Literature {.tabset .tabset-fade}
An algorithmic approach based on these observations is implemented below. Note that we are using the [median filter](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470740156) from the [fractal package](https://cran.r-project.org/web/packages/fractal/fractal.pdf) on CRAN. Note that in the analysis below, we are applying the *median Filter*, with n=29 (i.e., the median is calculated based on a overlaping moving window of size 29 strides). We chose this window size based on the recommendation of our biomechanics expert as it allows us to: (a) ignore the effect of any turns in the direction of motion; and (b) the corresponding window size is approximately 30 seconds long (which presents an easy to remember rule for practical application).
```{r SigProcOutlierDetection, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
# We are reading the data locally (you can download the file from GitHub)
load(file = "./Data/RGenerated/FeatureGeneration.RData")
for (i in 1:15) {
  df <- get(paste0("subject",i,"_features")) # Getting Data from Sec.
  # Obtaining the Outliers for each of the three variables
  time.from.start <- df[,1]
  scaled.stride.len <- medianFilter(df[,2], order = 29) # moving window of size 29 obs
  scaled.stride.height <- medianFilter(df[,3], order = 29) # moving window of size 29 obs
  stride.duration <- medianFilter(df[,4], order = 29) # moving window of size 29 obs
 
  # Computing the first difference to make seein the values easier
  diff.time.from.start <- time.from.start[-1]
  diff.scaled.stride.len <- diff(scaled.stride.len)
  diff.scaled.stride.height <- diff(scaled.stride.height)
  diff.stride.duration <- diff(stride.duration)
  # Remove the observations corresponding to the outliers
  assign(paste0("subject",i,"_medianf"), 
         data.frame(cbind(time.from.start, scaled.stride.len, 
                          scaled.stride.height, stride.duration)))
  
  assign(paste0("subject",i,"_median_diff_f"), 
         data.frame(cbind( diff.time.from.start, diff.scaled.stride.len, 
                          diff.scaled.stride.height, diff.stride.duration)))
  
  # Preparing the data for the Line Graph
  df_transformed <- melt(get(paste0("subject",i,"_medianf")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
  
  diff_df_transformed  <- melt(get(paste0("subject",i,"_median_diff_f")),
                         id.vars = "diff.time.from.start",
                         measure.vars=c("diff.scaled.stride.len",
                                        "diff.scaled.stride.height",
                                        "diff.stride.duration"
                                        )
                         ) # ggplot data needs to be tall

  assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Data post the application of the median filter") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  
  assign(paste0("h",i),
         ggplot(data = diff_df_transformed,
                aes(x=diff.time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("First Difference of the Signal post the median filter") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  
  cat('###',paste0("P", i), "{-}",'\n')
  
  print(get(paste0("g",i)))
  
  cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>median filter</b>, Participant ', i,
             ' has an average scaled stride length of ',
             round(mean(scaled.stride.len),2),
             ', an average scaled stride height of ',
             round(mean(scaled.stride.height),2),
             ' and an average stride duration of ', round(mean(stride.duration),2),
             ' seconds. Based on their height of ', subject_heights[i],
             ' meters, their average stride length and stride height are', ' ',
             round(mean(scaled.stride.len)*subject_heights[i],2),
             ' and ', ' ', round(mean(scaled.stride.height)*subject_heights[i],2),
             ', respectively.', '</p> </source>'
             )
      )
  
  cat('\n')
  
  print(get(paste0("h",i)))
  
  cat(paste0('<source> <p> Based on the <b>median filter </b>the interquarile range of the <b>first difference </b> of Participant ', i,
             '\'s scaled stride length, scaled stride height and stride duration are',
             ' ',
             round(IQR(diff.scaled.stride.len),3),
             ', ', round(IQR(diff.scaled.stride.height),3),
             ', and ', round(IQR(diff.stride.duration),3),
             ', respectively. </p> </source>'
             )
      )
    
  cat(' \n \n')
  cat('<source> <p> The data processed with the median filtered approach is stored at the <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/MedianFilteredData.RData">following location</a> in our GitHub Repository.')
  cat('\n \n')
  
}
# Saving a list of cleaned 
save(subject1_medianf, subject2_medianf, subject3_medianf, subject4_medianf,
     subject5_medianf, subject6_medianf, subject7_medianf, subject8_medianf,
     subject9_medianf, subject10_medianf, subject11_medianf, subject12_medianf,
     subject13_medianf, subject14_medianf, subject15_medianf,
     subject1_median_diff_f, subject2_median_diff_f, subject3_median_diff_f,
     subject4_median_diff_f,
     subject5_median_diff_f, subject6_median_diff_f, subject7_median_diff_f,
     subject8_median_diff_f,
     subject9_median_diff_f, subject10_median_diff_f, subject11_median_diff_f,
     subject12_median_diff_f,
     subject13_median_diff_f, subject14_median_diff_f, subject15_median_diff_f,
     file = "./Data/RGenerated/MedianFilteredData.RData")
```


## Applying the Median Filter Methodology on the CUSUM of the Features {.tabset .tabset-fade}
```{r CUSUM, fig.align='center', results='asis', out.width='65%', message=FALSE, cache=TRUE}
  load(file = "./Data/RGenerated/MedianFilteredData.RData")
  for (i in 1:15) {
    df <- get(paste0("subject",i,"_medianf"))
  means <- colMeans(df)
  df[,2] <- cumsum(df[,2]-means[2])
  df[,3] <- cumsum(df[,3]-means[3])
  df[,4] <- cumsum(df[,4]-means[4])
  assign(paste0("subject",i,"_cusum"), df)  
    
      cat('###',paste0("P", i), "{-}",'\n')
      df_transformed <- melt(get(paste0("subject",i,"_cusum")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
      
      assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("CUSUM of the Median Filtered Data") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") 
         )
      
      print(get(paste0("g",i)))
  
  cat('\n')
  
  cat('<source> <p> The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/CusumData.RData">CusumData.RData</a>. </p> </source>')
  cat('\n \n')
  }
save(subject1_cusum, subject2_cusum, subject3_cusum,
     subject4_cusum, subject5_cusum, subject6_cusum,
     subject7_cusum, subject8_cusum, subject9_cusum,
     subject10_cusum, subject11_cusum, subject12_cusum,
     subject13_cusum, subject14_cusum, subject15_cusum,
       file = "./Data/RGenerated/CusumData.RData")
```


# Univariate Changepoint Detection 
A common approach for changepoint detection is to project the multivariate problem into a univariate-space using *principal component analysis*. Then, apply a univariate changepoint detection approach on the problem. We examine this approach in this section.

## Applied to the PCs of the Median Filtered Kinematic Features {.tabset .tabset-fade}
When we tested this, we found the following:  
  (A) The e.divise() produces a lot of changepoints with the deafult sig.level;  
  (B) Number of changepoints is still large with sig.lvl = 0.001;  
  
For this reason, we do not think that this approach is promising. We will move on to the multivariate approach.

```{r pca, ig.align='center', results='asis', out.width='65%', message=FALSE, cache=TRUE}
RatingThresh <- 13  # based on Maman et al. 2017
RPE = read.csv("https://raw.githubusercontent.com/fmegahed/fatigue-changepoint/master/Data/Raw/RPE.csv") # to read subjective rating of RPE data from GitHub'
colnames(RPE)[1] <- "time.from.start"
df_RPE_transformed <- melt(RPE,
                           id.vars = "time.from.start",
                           measure.vars = colnames(RPE)[2:16])
df_RPE_transformed[,2] <- paste0(df_RPE_transformed[,2],".RPE")
# to load previously generated data and changepoints 
load(file = "./Data/RGenerated/MedianFilteredData.RData")
pca_subjects <- list()
pca_subjects_loadings <- list()
pc_prop_var_explained <- list()
changepoints_pca_ecp <- list()
changepoints_subj_thr <- list()
changepoints_subj_ecp <- list()
changepoints_cusum_subj_ecp <- list()

for (i in 1:15) {
  cumsum_RPE <- data.frame(matrix(ncol=3, nrow=18)) # need to initialize every time
  colnames(cumsum_RPE) <- c("time.from.start","variable","value")
  correct_subject_rows <- which(df_RPE_transformed[,2]==paste0("Subject.",i,".RPE"))
  dataholder <- df_RPE_transformed[correct_subject_rows,]
  # Computing the CUSUMs of the RPEs
  cumsum_RPE[,1] <- dataholder[,1]
  cumsum_RPE[,2] <- paste0(dataholder[,2],".CUSUM")
  cumsum_RPE[,3] <- cumsum(dataholder[,3]-13)
  cumsum_RPE <- na.omit(cumsum_RPE)
  
  # -------- Calculation of Fatigue based on RPE>= 13 -----------------
  fatigue_from_RPE_threshold <- which(RPE[,i+1]>=RatingThresh) %>% 
                              min() # returns window number
  changepoints_subj_thr[[i]] <- RPE[fatigue_from_RPE_threshold,1]
  df_RPE<-data.frame(d1=RPE[1],t1=RPE[i+1]) %>% na.omit() # to make dataframe of subjective ratings for ggplot
  
  #------- Estimation of Fatigue Changepoints based on ECP of the RPE Data --
  y <- e.divisive(df_RPE,min.size = 2)$estimates
  fatigue_from_ecp_changepoint = y[2:(length(y)-1)] # removing the first and last points as trivials changepoints
  changepoints_subj_ecp[[i]] <- df_RPE[fatigue_from_ecp_changepoint,1]
  
  # ------ Repeating the above analysis for the CUSUMs of the RPE --------
  y_cumsum <- e.divisive(cumsum_RPE[,c(1,3)],min.size = 2)$estimates
  fatigue_ecp_CusumRPE <- y_cumsum[2:length(y_cumsum)-1]
  changepoints_cusum_subj_ecp[[i]] <- cumsum_RPE[fatigue_ecp_CusumRPE, 1]
  
  # ------Computing the First Principal Component and the loadings for First PC ---
  df <- paste0("subject",i,"_medianf") %>% get()
  pca_dataholder <-  prcomp(df[,2:4], center=TRUE, scale. = T) # Scale <- Standardize Vars
  pca_subjects_loadings[[i]] <- pca_dataholder$rotation[,1] # Weights for First PC
  # Understanding the % Variation Explained by the First PC
  pc_std <- pca_dataholder$sdev
  pc_var <- pc_std^2
  pc_prop_var_explained[[i]] <- round(pc_var[1]/sum(pc_var),3)
  #  Storing the Data for First PC rotated values
  pca_subjects[[i]] <- data.frame(time.from.start= df[,1], 
                                variable= "PC1.Values",
                                value =pca_dataholder$x[,1]) # First PC
  df_transformed <- pca_subjects[[i]]
  df_transformed <- rbind(df_transformed,df_RPE_transformed[correct_subject_rows,],
                        cumsum_RPE)
  # The Changepoints for the PCA Data based on e.divisive
  # pca_df <- pca_subjects[[i]]
  # z <- e.divisive(pca_df[,c(1,3)],min.size = 120, sig.lvl = 0.001)$estimates
  # changepoints_pca_ecp[[i]] <- pca_df[z[2:(length(z)-1)],]
  
  cat('###',paste0("P", i), "{-}",'\n')
  assign(paste0("g",i), biplot(pca_dataholder, 
                               scale = 0))
  print(get(paste0("g",i)))
 
  cat("\n")
  # pca_vector <- pca_df[,c(1)]
  
  assign(paste0("h",i),
        ggplot(data = df_transformed,
               aes(x=time.from.start, y=value, colour = variable)) + 
          geom_line(size=1) +
          theme_bw() +
          scale_colour_manual(values = c("maroon","black","grey40")) +
          facet_wrap(~variable,nrow=3, scales = "free_y") +
          # labs(title = paste("A paneled time-series plot for Participant",i)) + 
          # geom_vline(xintercept= pca_vector, 
          #            show.legend = T, size=0.5) + 
          geom_vline(xintercept= changepoints_subj_thr[[i]],
                     linetype="solid", color="steelblue1",
                     show.legend = T, size=2) +
          geom_vline(xintercept= changepoints_subj_ecp[[i]],
                     linetype="dotted", color="gray80",
                     show.legend =T, size=2) +
          geom_vline(xintercept= changepoints_cusum_subj_ecp[[i]],
                     linetype="dashed", color="orange1",
                     show.legend =T, size=3) +
          theme(legend.position="none",
                 plot.title = element_text(hjust = 0.5))
      )
      
    print(get(paste0("h",i)))
  
    cat('\n \n')
    
    # First, the <b> black  lines are used to denote the changepoints based on the non-parameteric, univariate, ECP approach based on the e.divisive(min.size=2) on the First Principal Component of the Kinematic Features. </b> Second
  
    cat(paste0('<source> <p> In the above figure, the colored vertical lines correspond to the following.  First, the <font color= #63B8FF> <b>steelblue line  correspond to the time from start in seconds, when the subject has a RPE that equals or exceeds 13. </b> </font> Note that we only indicate the first occurence when the RPE>=13 on the graphs. Second, the <b> <font color=#CCCCCC> gray lines  denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the RPE values from the subject. </font> </b>',
'. Third, the <b> <font color=#FFA500> orange lines  are used to denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the CUSUM of the RPE values. </font> </b> \n \n'))
  
# <source> <p> Based on the <b>ecp package, the number of changepoints for the First PC of the median filtered data for participant ', i,
 #            ' is equal to: ',
             # length(changepoints_pca_ecp[[i]]),
             # '. These changepoints are located at: ',
             # paste(changepoints_pca_ecp[[i]], collapse = ", "),
#             '.  <font color= #63B8FF>    
    
    cat(paste0('<source> <p>  <font color= #63B8FF> <b> The threshold based changepoint in the subjective RPE is located at: ',
             paste(changepoints_subj_thr[[i]]),
             '. </font> </b> In addition, <b> <font color=#CCCCCC> the number of changepoints based on the RPE values for this subject is equal to: ',
             length(changepoints_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_subj_ecp[[i]], collapse = ", "),
             ". </font> </b> Similar to our previous changepoint analysis, we also obtained the changepoints for the CUSUMs of the RPEs. <b> <font color=#FFA500> The number of these changepoints is equal to: ",
             length(changepoints_cusum_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_cusum_subj_ecp[[i]], collapse = ", "),
             '. </b> </font> The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/PCAChangepointsRPE.RData">PCAChangepointsRPE.RData</a>. </p> </source>')
      )
  
  cat('\n \n')
}

save(pca_subjects, pca_subjects_loadings, pc_prop_var_explained,
     changepoints_pca_ecp, 
     changepoints_subj_thr, changepoints_subj_ecp, changepoints_cusum_subj_ecp,
     file = "./Data/RGenerated/PCAChangepointsRPE.RData.RData")

```


# Multivariate Changepoint Detection
To be filled later.

## The Nonparameteric Approach of Matterson & James (ECP) 
In this section, we will examine the use of the approach of [Matteson and James (2014)](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.849605) for multiple change point analysis of our trivariate data. Our analysis takes advantage of their **R** package titled *ecp*, which is described in more details in the following paper: [James and Matteson (2014)](https://cran.r-project.org/web/packages/ecp/vignettes/ecp.pdf). The multivariate changepoint method will be first tested using the median filtered data and then using the CUSUM of the median filtered data.

### Using the Median Filtered Data {.tabset .tabset-fade}
To apply the *ecp approach of [Matteson and James (2014)](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.849605)* on the **median filtered data**, we need to use a window size (which reflects the smallest size of a shift that is to be detected). From a biomechanics perspective, we expect that the change of performance due to fatigue will be sustained at least for several minutes (until the person adapts and/or rests). Thus, the size of the window ($\small m_{ecp}$) should be $\small m_{ecp} \ge 60$. In our analysis, we tried the following window sizes: 60, 120, 180, 300, and 600. For most of the participants, the number and/or location of change were almost similar. For demonstration purpose, we use $\small m_{ecp} = 120$ below.  
```{r MCP_MedianFilteredData, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
  load(file = "./Data/RGenerated/MedianFilteredData.RData")
  pen <- function(x) -length(x) #Equally penalizes every additional changepoint
  window.size <- 120
  changepoints_medianf <- list()

  for (i in 1:15) {
      df <- get(paste0("subject",i,"_medianf"))
      # Making the DF divisiable by the window size
      # (i.e., we are removing the remainder so that our chunks are equally sized)
      rows.to.keep <- nrow(df[,2:4]) %/% window.size
      remainder <- nrow(df[,2:4]) %% window.size
      df.MultivariateChangepoint<- df[1:(nrow(df)-remainder),
                                      2:4] %>% as.matrix()
      
      # Utilizing the e.agglo() from the ecp package
      mem <- rep(1:rows.to.keep, each=window.size)
      y = e.agglo(X=df.MultivariateChangepoint,
                  member= mem,
                  alpha = 1,
                  penalty = pen)
      fatigue_from_ECP <- y$estimates # Returns observation number
      changepoints_medianf[[i]] <- df[fatigue_from_ECP,1]
      
      cat('####',paste0("P", i), "{-}",'\n')
      df_transformed <- melt(get(paste0("subject",i,"_medianf")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
      
      assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Changepoints for the Median Filtered Data (at vertical black lines)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") +
           geom_vline(xintercept= changepoints_medianf[[i]])
         )
      
      print(get(paste0("g",i)))
  
  cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>ecp package</b>, the number of changepoints for participant ', i,
             ' is equal to: ',
             length(changepoints_medianf[[i]]),
             '. These changepoints are located at: ',
             paste(round(changepoints_medianf[[i]]), collapse = ", "),
             '. The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ECPChangePointsMFData.RData">ECPChangePointsMFData.RData</a>. </p> </source>')
      )
  cat('\n \n')
  }
save(changepoints_medianf,
       file = "./Data/RGenerated/ECPChangePointsMFData.RData")
```


### Using the Median Filtered Differenced Data {.tabset .tabset-fade}
Here, we apply the ecp approach of [Matteson and James (2014)](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.849605) on the **median filtered differenced data** (as opposed to the median filtered timeseries). For demonstration purpose, we use $\small m_{ecp} = 120$ below. ** Note that, here, we set the value of $\small \alpha=2$ as opposed to 1 (elsewhere) since we are looking for changes in the variation instead of the mean. See their paper for more details.   
```{r MCP_MedianFilteredDiffData, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
  load(file = "./Data/RGenerated/MedianFilteredData.RData")
  pen <- function(x) -length(x) #Equally penalizes every additional changepoint
  window.size <- 120
  changepoints_medianf_diff <- list()

  for (i in 1:15) {
      df <- get(paste0("subject",i,"_median_diff_f"))
      # Making the DF divisiable by the window size
      # (i.e., we are removing the remainder so that our chunks are equally sized)
      rows.to.keep <- nrow(df[,2:4]) %/% window.size
      remainder <- nrow(df[,2:4]) %% window.size
      df.MultivariateChangepoint<- df[1:(nrow(df)-remainder),
                                      2:4] %>% as.matrix()
      
      # Utilizing the e.agglo() from the ecp package
      mem <- rep(1:rows.to.keep, each=window.size)
      y = e.agglo(X=df.MultivariateChangepoint,
                  member= mem,
                  alpha = 2,
                  penalty = pen)
      fatigue_from_ECP <- y$estimates
      changepoints_medianf_diff[[i]] <- df[fatigue_from_ECP,1]
      
      cat('####',paste0("P", i), "{-}",'\n')
      df_transformed <- melt(get(paste0("subject",i,"_median_diff_f")),
                         id.vars = "diff.time.from.start",
                         measure.vars=c("diff.scaled.stride.len",
                                        "diff.scaled.stride.height",
                                        "diff.stride.duration"
                                        )
                         ) # ggplot data needs to be tall
      
      assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=diff.time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Changepoints for the Median Filtered Differenced Data (at vertical black lines)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") +
           geom_vline(xintercept= changepoints_medianf_diff[[i]])
         )
      
      print(get(paste0("g",i)))
  
  cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>ecp package</b>, the number of changepoints for the median filtered differenced data for participant ', i,
             ' is equal to: ',
             length(changepoints_medianf_diff[[i]]),
             '. These changepoints are located at: ',
             paste(round(changepoints_medianf_diff[[i]]), collapse = ", "),
             '. The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ECPChangePointsDiffMFData.RData">ECPChangePointsDiffMFData.RData</a>. </p> </source>')
      )
  
  cat('\n \n')

  }
save(changepoints_medianf_diff,
       file = "./Data/RGenerated/ECPChangePointsDiffMFData.RData")
```


### Using the CUSUM of the Median Filtered Data {.tabset .tabset-fade}
Here, we apply the ecp approach of Matterson and James (2014) on the **cusums of the median filtered data** (as opposed to the median filtered timeseries). For demonstration purpose, we use $\small m_{ecp} = 120$ below.  
```{r MCP_CUSUM, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
load(file = "./Data/RGenerated/CusumData.RData")
  pen <- function(x) -length(x) #Equally penalizes every additional changepoint
  window.size <- 120
  changepoints_cusum <- list()

  for (i in 1:15) {
      df <- get(paste0("subject",i,"_cusum"))
      # Making the DF divisiable by the window size
      # (i.e., we are removing the remainder so that our chunks are equally sized)
      rows.to.keep <- nrow(df[,2:4]) %/% window.size
      remainder <- nrow(df[,2:4]) %% window.size
      df.MultivariateChangepoint<- df[1:(nrow(df)-remainder),
                                      2:4] %>% as.matrix()
      
      # Utilizing the e.agglo() from the ecp package
      mem <- rep(1:rows.to.keep, each=window.size)
      y = e.agglo(X=df.MultivariateChangepoint,
                  member= mem,
                  alpha = 1,
                  penalty = pen)
      fatigue_from_CUSUM_ECP <- y$estimates
      changepoints_cusum[[i]] <- df[fatigue_from_CUSUM_ECP,1]
      
      cat('####',paste0("P", i), "{-}",'\n')
      df_transformed <- melt(get(paste0("subject",i,"_cusum")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
      
      assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Changepoints for the CUSUMs of the Median Filtered Data (vertical black lines)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") +
           geom_vline(xintercept= changepoints_cusum[[i]])
         )
      
      print(get(paste0("g",i)))
  
  cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>ecp package</b>, the number of changepoints for the cusums of the median filtered data for participant ', i,
             ' is equal to: ',
             length(changepoints_cusum[[i]]),
             '. These changepoints are located at: ',
             paste(round(changepoints_cusum[[i]]), collapse = ", "),
             '. The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ECPChangePointsCUSUM.RData">ECPChangePointsCUSUM.RData</a>. </p> </source>')
      )
  cat('\n \n')
  }
save(changepoints_cusum,
       file = "./Data/RGenerated/ECPChangePointsCUSUM.RData")
```

## The Approach of Capizzi and Masarotto (dfphase1)
To compare the performance of the ECP approach, we also investigated using the approach of [Cappizi and Masarotto (2016)](https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2016.1272494) for another nonparameteric multivariate changepoint analysis method. Similar to the *ecp approach*, we applied their methodology on the: (a) median filtered data; and (b) cusums of the median filtered data. The results are shown in the following subsections. **Note that we did not perform the analysis on the differences since we concluded that it was less informative than (a) and (b) based on our conclusions from Section 4.1.** 

For the parameterization of the [Cappizi and Masarotto (2016)](https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2016.1272494) approach, we used the default parameters of the [mphase1() function from the dfphase1 package](https://cran.r-project.org/web/packages/dfphase1/dfphase1.pdf) with the exception of:  
  -- *isolated = FALSE* since we do not expect shifts to last only 120 strides.  
  -- *plot = FALSE* since we would like to use *ggplot package* instead of the default plot function.

Notes:  

  (A) Similar to the analysis in Section 4.1, we are using the window size of 120 ($\small m_{dphase1}=120$). This allows us to compare the results (in terms of the number of detected changepoints and whether these match better from a graphical perspective).  
  
  (B) Note that we have used the default $\small lmin=5$, which means that the mphase1 method will ignore any changepoints within 5 windows of the previously detected changepoint. While this is different than the ECP method, we still have more changepoints (and thus, we have elected against changing this default setting).

### Using the Median Filtered Data {.tabset .tabset-fade}
```{r MPH1_MF, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
  load(file = "./Data/RGenerated/MedianFilteredData.RData")
  changepoints_mph1_mf <- list()

  for (i in 1:15) {
    df <- get(paste0("subject",i,"_medianf"))
    window.size <- 120
    rows.to.keep <- nrow(df[,2:4]) %/% window.size
    remainder <- nrow(df[,2:4]) %% window.size
    df.MultivariateChangepoint<- df[1:(nrow(df)-remainder),
                                    2:4]
    dataholder.array <- t(df.MultivariateChangepoint) %>% 
                          array(c(3, window.size, rows.to.keep))
    y <- mphase1(dataholder.array,
                 isolated=FALSE, plot = FALSE, alpha=0.001)
    mcp <- (df[unlist(y$alasso[2]),1]*window.size) %>% round(digits = 0)
    mcp <- mcp[order(mcp)]
    mcp <- (mcp - window.size) + 1 # to make the change point at the begining of the period and not end (to be similar to the ecp method)
    changepoints_mph1_mf[[i]] <- y
    
     cat('####',paste0("P", i), "{-}",'\n')
    df_transformed <- melt(get(paste0("subject",i,"_medianf")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                         )
  ) # ggplot data needs to be tall
  
  assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Changepoints for the Median Filtered Data (w/ dfphase1 package)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") +
           geom_vline(xintercept= mcp)
  )
  
  print(get(paste0("g",i)))
  
 cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>dfphase1 package</b>, the number of changepoints for the cusums of the median filtered data for participant ', i,
             ' is equal to: ',
             length(mcp),
             '. These changepoints are located at: ',
             paste(mcp, collapse = ", "),
             '. The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ChangePointsMedianFilterMphase1.RData">ChangePointsMedianFilterMphase1.RData</a>. </p> </source>')
  )
  cat('\n \n')
  }
  save(changepoints_mph1_mf,
       file = "./Data/RGenerated/ChangePointsMedianFilterMphase1.RData")
```


### Using the CUSUM Data {.tabset .tabset-fade}
```{r MPH1_CUSUM, cache=TRUE, fig.align='center', results='asis', out.width='65%', message=FALSE}
  load(file = "./Data/RGenerated/CusumData.RData")
  changepoints_mph1_cusum <- list()

  for (i in 1:15) {
    df <- get(paste0("subject",i,"_cusum"))
    window.size <- 120
    rows.to.keep <- nrow(df[,2:4]) %/% window.size
    remainder <- nrow(df[,2:4]) %% window.size
    df.MultivariateChangepoint<- df[1:(nrow(df)-remainder),
                                    2:4]
    dataholder.array <- t(df.MultivariateChangepoint) %>% 
                          array(c(3, window.size, rows.to.keep))
    y <- mphase1(dataholder.array,
                 isolated=FALSE, plot = FALSE, alpha=0.001)
    mcp <- (df[unlist(y$alasso[2]),1]*window.size) %>% round(digits = 0)
    mcp <- mcp[order(mcp)]
    mcp <- (mcp - window.size) + 1 # to make the change point at the begining of the period and not end (to be similar to the ecp method)
    changepoints_mph1_cusum[[i]] <- y
    
     cat('####',paste0("P", i), "{-}",'\n')
  df_transformed <- melt(get(paste0("subject",i,"_cusum")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                         )
  ) # ggplot data needs to be tall
  
  assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Changepoints for the CUSUMs of the Median Filtered Data (w/ dfphase1 package)") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y") +
           geom_vline(xintercept= mcp)
  )
  
  print(get(paste0("g",i)))
  
 cat('\n')
  
  cat(paste0('<source> <p> Based on the <b>dfphase1 package</b>, the number of changepoints for the cusums of the median filtered data for participant ', i,
             ' is equal to: ',
             length(mcp),
             '. These changepoints are located at: ',
             paste(mcp, collapse = ", "),
             '. The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ChangePointsCusumMphase1.RData">ChangePointsCusumMphase1.RData</a>. </p> </source>')
  )
  cat('\n \n')
  }
  save(changepoints_mph1_cusum,
       file = "./Data/RGenerated/ChangePointsCusumMphase1.RData")
```


## Comments on the ECP and dfphase1 Methods
Based on the results in Sections 4.1 and 4.2, the approach of [Matterson and James (2016)](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.849605) is preferred since it:

  (A) Results in a more limited number of changepoints, highlighting more (practically) significant changes in the gait parameters. This result was somewhat surprising since we used the default settings for the approach of [Capizzi and Masarotto (2016)](https://amstat.tandfonline.com/doi/abs/10.1080/00401706.2016.1272494) throught the default [mphase1()](https://cran.r-project.org/web/packages/dfphase1/dfphase1.pdf), which prevents having changepoints within 5 windows. Note that this result holds true even when we adjusted the $\small \alpha =0.001$ instead of the default of $\small \alpha =0.05$:) for the mphase1() method (without adjusting the ecp() method since it does not allow for adjusting the false alarm rate); and  
  
  (B) Is more computationally efficient. It had a shorter run time than the other approach.
  
In addition, from the analysis, the changepoints based on the CUSUM of the median filtered data are the most insightful. Specifically, one can discern how each participant's performance change as the experiment goes on. For example, P10 has a clear fatigued performance in the end, where the cusums of the stride length and stride height are decreasing rapidly after stride 5521.

# Comparing the ECP Changepoints with Participants' Subjective Ratings  
Throughout the experiments, participants provided their subjective ratings every 10 minutes. The ratings were based on the [Borg 6-20 RPE scale](https://www.tandfonline.com/doi/abs/10.1080/00140139.2018.1442936). Recent literature on fatigue detection suggests that a rating of 13 can be used to estimate the onset of fatigue (see [Maman et al. (2017)](https://www.sciencedirect.com/science/article/pii/S0003687017300261) for a detailed analysis). The subjective ratings for each participant can be accessed at: [subjective-ratings-raw](https://github.com/fmegahed/fatigue-changepoint/tree/master/Data/Raw).


## Plots with the Median Filtered Data and their ECP Changepoints {.tabset .tabset-fade}
In this section, we perform the following tasks:  

(A) EDA, where we overlay the subjective ratings with the **Median Filtered changepoints**;

(B) A threshold based comparison, where we use $\small RPE \ge 13$ to identify when a participant becomes fatigued and we compare that to the changepoints detected by the hybrid CUSUM-EPC approach; and 

(C) Utilize the [e.divisive() from the ecp package](https://cran.r-project.org/web/packages/ecp/vignettes/ecp.pdf) to obtain the changepoints for the Borg Ratings. Note that this function is the univariate approach for the multiple changepoint method examined in Section 4.1. 

```{r RPE_AnalysisMF, cache=TRUE, fig.align='center', results='asis', out.width='100%', message=FALSE, warning=FALSE}
RatingThresh <- 13  # based on Maman et al. 2017 
RPE = read.csv("https://raw.githubusercontent.com/fmegahed/fatigue-changepoint/master/Data/Raw/RPE.csv") # to read subjective rating of RPE data from GitHub'
colnames(RPE)[1] <- "time.from.start"
df_RPE_transformed <- melt(RPE,
                           id.vars = "time.from.start",
                           measure.vars = colnames(RPE)[2:16])
df_RPE_transformed[,2] <- paste0(df_RPE_transformed[,2],".RPE")
# to load previously generated data and changepoints 
load(file = "./Data/RGenerated/MedianFilteredData.RData")
load(file = "./Data/RGenerated/ECPChangePointsMFData.RData")
changepoints_subj_ecp <- list()
changepoints_subj_thr <- list()
changepoints_cusum_subj_ecp <- list()

for (i in 1:15) {
  cumsum_RPE <- data.frame(matrix(ncol=3, nrow=18)) # need to initialize every time
  colnames(cumsum_RPE) <- c("time.from.start","variable","value")
  correct_subject_rows <- which(df_RPE_transformed[,2]==paste0("Subject.",i,".RPE"))
  dataholder <- df_RPE_transformed[correct_subject_rows,]
  # Computing the CUSUMs of the RPEs
  cumsum_RPE[,1] <- dataholder[,1]
  cumsum_RPE[,2] <- paste0(dataholder[,2],".CUSUM")
  cumsum_RPE[,3] <- cumsum(dataholder[,3]-13)
  cumsum_RPE <- na.omit(cumsum_RPE)
  
  df_transformed <- rbind(df_transformed,df_RPE_transformed[correct_subject_rows,],
                            cumsum_RPE)
  
  # -------- Calculation of Fatigue based on RPE>= 13 -----------------
  fatigue_from_RPE_threshold <- which(RPE[,i+1]>=RatingThresh) %>% 
                              min() # returns window number
  changepoints_subj_thr[[i]] <- RPE[fatigue_from_RPE_threshold,1]
  df_RPE<-data.frame(d1=RPE[1],t1=RPE[i+1]) %>% na.omit() # to make dataframe of subjective ratings for ggplot
  
  #------- Estimation of Fatigue Changepoints based on ECP of the RPE Data --
  y <- e.divisive(df_RPE,min.size = 2)$estimates
  fatigue_from_ecp_changepoint = y[2:(length(y)-1)] # removing the first and last points as trivials changepoints
  changepoints_subj_ecp[[i]] <- df_RPE[fatigue_from_ecp_changepoint,1]
  
  # ------ Repeating the above analysis for the CUSUMs of the RPE --------
  y_cumsum <- e.divisive(cumsum_RPE[,c(1,3)],min.size = 2)$estimates
  fatigue_ecp_CusumRPE <- y_cumsum[2:length(y_cumsum)-1]
  changepoints_cusum_subj_ecp[[i]] <- cumsum_RPE[fatigue_ecp_CusumRPE, 1]
  
  cat('###',paste0("P", i), "{-}",'\n')
  df_transformed <- melt(get(paste0("subject",i,"_medianf")),
                       id.vars = "time.from.start",
                       measure.vars=c("scaled.stride.len",
                                      "scaled.stride.height",
                                      "stride.duration"
                                      )
                         )
  df_transformed <- rbind(df_transformed,df_RPE_transformed[correct_subject_rows,],
                            cumsum_RPE)
      
  assign(paste0("g",i),
        ggplot(data = df_transformed,
               aes(x=time.from.start, y=value, colour = variable)) + 
          geom_line(size=1) +
          theme_bw() +
          scale_colour_manual(values = c("red", "green", "blue","black","grey40")) +
          facet_wrap(~variable,nrow=5, scales = "free_y") +
          # labs(title = paste("A paneled time-series plot for Participant",i)) + 
          geom_vline(xintercept= changepoints_medianf[[i]], 
                     show.legend = T, size=2) + 
          geom_vline(xintercept= changepoints_subj_thr[[i]],
                     linetype="solid", color="steelblue1",
                     show.legend = T, size=2) +
          geom_vline(xintercept= changepoints_subj_ecp[[i]],
                     linetype="dotted", color="gray80",
                     show.legend =T, size=2) +
          geom_vline(xintercept= changepoints_cusum_subj_ecp[[i]],
                     linetype="dashed", color="orange1",
                     show.legend =T, size=3) +
          theme(legend.position="none",
                 plot.title = element_text(hjust = 0.5))
      )
      
      print(get(paste0("g",i)))
  
  cat('\n \n')
  
  cat(paste0('<source> <p> In the above figure, the colored vertical lines correspond to the following.  First, the <b> black  lines are used to denote the changepoints based on the non-parameteric, multivariate, ECP approach on the Kinematic Features. </b> These changepoints are what we obtained in Section 4.1.3. Second, the <font color= #63B8FF> <b>steelblue line  correspond to the time from start in seconds, when the subject has a RPE that equals or exceeds 13. </b> </font> Note that we only indicate the first occurence when the RPE>=13 on the graphs. Third, the <b> <font color=#CCCCCC> gray lines  denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the RPE values from the subject. </font> </b>',
'. Fourth, the <b> <font color=#FFA500> orange lines  are used to denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the CUSUM of the RPE values. </font> </b> \n \n'))
  
  cat(paste0('<source> <p> Based on the <b>ecp package, the number of changepoints for the the median filtered data for participant ', i,
             ' is equal to: ',
             length(changepoints_medianf[[i]]),
             '. These changepoints are located at: ',
             paste(round(changepoints_medianf[[i]]), collapse = ", "),
             '.  <font color= #63B8FF> The threshold based changepoint in the subjective RPE is located at: ',
             paste(changepoints_subj_thr[[i]]),
             '. </font> </b> In addition, <b> <font color=#CCCCCC> the number of changepoints based on the RPE values for this subject is equal to: ',
             length(changepoints_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_subj_ecp[[i]], collapse = ", "),
             ". </font> </b> Similar to our previous changepoint analysis, we also obtained the changepoints for the CUSUMs of the RPEs. <b> <font color=#FFA500> The number of these changepoints is equal to: ",
             length(changepoints_cusum_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_cusum_subj_ecp[[i]], collapse = ", "),
             '. </b> </font> The results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ChangepointsRPE.RData">ChangepointsRPE.RData</a>. </p> </source>')
      )
  
  cat('\n \n')
  
  }

save(changepoints_subj_ecp, changepoints_subj_thr, changepoints_cusum_subj_ecp,
     file = "./Data/RGenerated/ChangepointsRPE.RData")

```


## Plots with the CUSUM of the Median Filtered Data and the ECP-CUSUM Changepoints {.tabset .tabset-fade}
In this section, we perform the following tasks:  

(A) EDA, where we overlay the subjective ratings with the **CUSUM changepoints**;

(B) A threshold based comparison, where we use $\small RPE \ge 13$ to identify when a participant becomes fatigued and we compare that to the changepoints detected by the hybrid CUSUM-EPC approach; and 

(C) Utilize the [e.divisive() from the ecp package](https://cran.r-project.org/web/packages/ecp/vignettes/ecp.pdf) to obtain the changepoints for the Borg Ratings. Note that this function is the univariate approach for the multiple changepoint method examined in Section 4.1. 
```{r RPE_Analysis, cache=TRUE, fig.align='center', results='asis', out.width='100%', message=FALSE, warning=FALSE}
RatingThresh <- 13  # based on Maman et al. 2017 
RPE = read.csv("https://raw.githubusercontent.com/fmegahed/fatigue-changepoint/master/Data/Raw/RPE.csv") # to read subjective rating of RPE data from GitHub'
colnames(RPE)[1] <- "time.from.start"
df_RPE_transformed <- melt(RPE,
                           id.vars = "time.from.start",
                           measure.vars = colnames(RPE)[2:16])
df_RPE_transformed[,2] <- paste0(df_RPE_transformed[,2],".RPE")
# to load previously generated data and changepoints 
load(file = "./Data/RGenerated/CusumData.RData")
load(file = "./Data/RGenerated/ECPChangePointsCUSUM.RData")
changepoints_subj_ecp <- list()
changepoints_subj_thr <- list()
changepoints_cusum_subj_ecp <- list()

for (i in 1:15) {
  cumsum_RPE <- data.frame(matrix(ncol=3, nrow=18)) # need to initialize every time
  colnames(cumsum_RPE) <- c("time.from.start","variable","value")
  correct_subject_rows <- which(df_RPE_transformed[,2]==paste0("Subject.",i,".RPE"))
  dataholder <- df_RPE_transformed[correct_subject_rows,]
  # Computing the CUSUMs of the RPEs
  cumsum_RPE[,1] <- dataholder[,1]
  cumsum_RPE[,2] <- paste0(dataholder[,2],".CUSUM")
  cumsum_RPE[,3] <- cumsum(dataholder[,3]-13)
  cumsum_RPE <- na.omit(cumsum_RPE)
  
  
  # -------- Calculation of Fatigue based on RPE>= 13 -----------------
  fatigue_from_RPE_threshold <- which(RPE[,i+1]>=RatingThresh) %>% 
                              min() # returns window number
  changepoints_subj_thr[[i]] <- RPE[fatigue_from_RPE_threshold,1]
  df_RPE<-data.frame(d1=RPE[1],t1=RPE[i+1]) %>% na.omit() # to make dataframe of subjective ratings for ggplot
  
  #------- Estimation of Fatigue Changepoints based on ECP of the RPE Data --
  y <- e.divisive(df_RPE,min.size = 2)$estimates
  fatigue_from_ecp_changepoint = y[2:(length(y)-1)] # removing the first and last points as trivials changepoints
  changepoints_subj_ecp[[i]] <- df_RPE[fatigue_from_ecp_changepoint,1]
  
  # ------ Repeating the above analysis for the CUSUMs of the RPE --------
  y_cumsum <- e.divisive(cumsum_RPE[,c(1,3)],min.size = 2)$estimates
  fatigue_ecp_CusumRPE <- y_cumsum[2:length(y_cumsum)-1]
  changepoints_cusum_subj_ecp[[i]] <- cumsum_RPE[fatigue_ecp_CusumRPE, 1]
   
  cat('###',paste0("P", i), "{-}",'\n')
  df_transformed <- melt(get(paste0("subject",i,"_cusum")),
                       id.vars = "time.from.start",
                       measure.vars=c("scaled.stride.len",
                                      "scaled.stride.height",
                                      "stride.duration"
                                      )
                         )
  df_transformed <- rbind(df_transformed,df_RPE_transformed[correct_subject_rows,],
                            cumsum_RPE)
  
  assign(paste0("g",i),
        ggplot(data = df_transformed,
               aes(x=time.from.start, y=value, colour = variable)) + 
          geom_line(size=1) +
          theme_bw() +
          scale_colour_manual(values = c("red", "green", "blue","black","grey40")) +
          facet_wrap(~variable,nrow=5, scales = "free_y") +
          # labs(title = paste("A paneled time-series plot for Participant",i)) + 
          geom_vline(xintercept= changepoints_cusum[[i]], 
                     show.legend = T, size=2) + 
          geom_vline(xintercept= changepoints_subj_thr[[i]],
                     linetype="solid", color="steelblue1",
                     show.legend = T, size=2) +
          geom_vline(xintercept= changepoints_subj_ecp[[i]],
                     linetype="dotted", color="gray80",
                     show.legend =T, size=2) +
          geom_vline(xintercept= changepoints_cusum_subj_ecp[[i]],
                     linetype="dashed", color="orange1",
                     show.legend =T, size=3) +
          theme(legend.position="none",
                 plot.title = element_text(hjust = 0.5))
      )
      
      print(get(paste0("g",i)))
  
  cat('\n \n')
  
  cat(paste0('<source> <p> In the above figure, the colored vertical lines correspond to the following.  First, the <b> black  lines are used to denote the changepoints based on the non-parameteric, multivariate, ECP approach on the CUSUMS of the Kinematic Features. </b> These changepoints are what we obtained in Section 4.1.3. Second, the <font color= #63B8FF> <b>steelblue line  correspond to the time from start in seconds, when the subject has a RPE that equals or exceeds 13. </b> </font> Note that we only indicate the first occurence when the RPE>=13 on the graphs. Third, the <b> <font color=#CCCCCC> gray lines  denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the RPE values from the subject. </font> </b>',
'. Fourth, the <b> <font color=#FFA500> orange lines  are used to denote the univariate, nonparametric changepoints obtained from applying the e.divisive() on the CUSUM of the RPE values. </font> </b> \n \n'))
  
  cat(paste0('<source> <p> Based on the <b>ecp package, the number of changepoints for the cusums of the median filtered data for participant ', i,
             ' is equal to: ',
             length(changepoints_cusum[[i]]),
             '. These changepoints are located at: ',
             paste(round(changepoints_cusum[[i]]), collapse = ", "),
             '.  <font color= #63B8FF> The threshold based changepoint in the subjective RPE is located at: ',
             paste(changepoints_subj_thr[[i]]),
             '. </font> </b> In addition, <b> <font color=#CCCCCC> the number of changepoints based on the RPE values for this subject is equal to: ',
             length(changepoints_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_subj_ecp[[i]], collapse = ", "),
             ". </font> </b> Similar to our previous changepoint analysis, we also obtained the changepoints for the CUSUMs of the RPEs. <b> <font color=#FFA500> The number of these changepoints is equal to: ",
             length(changepoints_cusum_subj_ecp[[i]]),
             ". These changepoints are located at: ",
             paste(changepoints_cusum_subj_ecp[[i]], collapse = ", "),
             '. </b> </font> Note that the results from the analysis above can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/ChangepointsRPE.RData">ChangepointsRPE.RData</a>. This is the same link as that provided in the previous section since the RPEresults are identical. The change here was overlaying the RPE results with the CUSUM of the median filtered data and their corresponding changepoints from the ECP package (instead of directly with the median filtered data and their changepoints [also from the ECP package]). </p> </source>')
      )
  
  cat('\n \n')
  
  }
save(changepoints_subj_thr,changepoints_subj_ecp, changepoints_cusum_subj_ecp,
       file = "./Data/RGenerated/ChangepointsRPE.RData")
```


# Time-Series Clustering

## Using %Time-from-Start to Standardize the Length of Each Participant's Time-series {.tabset .tabset-fade}
In this section, we rescale/normalize the timescale a 0-100% scale for all participants to prepare for a timeseries clustering type analysis. Specifically, we sampled the three features from the data based on a 0.05% from start. This resulted in a standardization of the number of observations within each timeseries across our 15 paricipants. Based on our increment size, each timeseries is now comprised of 2000 observations, each at 0.05% time increment from its neighouring data points.

```{r Data_normalization, cache=TRUE, fig.align='center', results='asis', out.width='100%', message=FALSE, warning=FALSE}
# to load previously generated data 
load(file = "./Data/RGenerated/FeatureGeneration.RData")

medianfilt_norm <- list() 
cusum_norm <- list()

for (i in 1:15) {
  
  df <- get(paste0("subject",i,"_features")) # Getting Data from Sec.
  
  # resampling the data to 2000 samples
  nsamples = 2000
  timePerc <- seq(0, 100, by=0.05)
  timeScale <- df[nrow(df),1]/nsamples
  df_norm = df[FALSE,]
  
  for (j in 1:nsamples) {
    df_matchIsx <- match.closest(timeScale*(j-0.5),df[,1]) # finding the closest point
    df_keep <- df[df_matchIsx,]
    df_keep[,1] <- timePerc[j]
    df_norm <- rbind(df_norm, df_keep)
  }
  assign(paste0("subject",i,"_features_norm"), df_norm)
  
  # median filter  
  df <- get(paste0("subject",i,"_features_norm")) # Getting Data from Sec.
  # Obtaining the Outliers for each of the three variables
  time.from.start <- df[,1]
  scaled.stride.len <- medianFilter(df[,2], order = 29) # moving window of size 29 obs
  scaled.stride.height <- medianFilter(df[,3], order = 29) # moving window of size 29 obs
  stride.duration <- medianFilter(df[,4], order = 29) # moving window of size 29 obs
  assign(paste0("subject",i,"_medianf_norm"), 
         data.frame(cbind(time.from.start, scaled.stride.len, 
                          scaled.stride.height, stride.duration)))
  medianfilt_norm[[i]] <- get(paste0("subject",i,"_medianf_norm"))

  
  # cusum calculation
  df <- get(paste0("subject",i,"_medianf_norm"))
  means <- colMeans(df)
  df[,2] <- cumsum(df[,2]-means[2])
  df[,3] <- cumsum(df[,3]-means[3])
  df[,4] <- cumsum(df[,4]-means[4])
  assign(paste0("subject",i,"_cusum_norm"), df)  
  cusum_norm[[i]] <- get(paste0("subject",i,"_cusum_norm"))
  
  
   # Preparing the data for the Line Graph
  df_transformed <- melt(get(paste0("subject",i,"_features_norm")),
                         id.vars = "time.from.start",
                         measure.vars=c("scaled.stride.len",
                                        "scaled.stride.height",
                                        "stride.duration"
                                        )
                         ) # ggplot data needs to be tall
  
  median_df_transformed <- melt(get(paste0("subject",i,"_medianf_norm")),
                                 id.vars = "time.from.start",
                                 measure.vars=c("scaled.stride.len",
                                                "scaled.stride.height",
                                                "stride.duration"
                                                )
                                 ) # ggplot data needs to be tall
          
  cusum_df_transformed  <- melt(get(paste0("subject",i,"_cusum_norm")),
                                id.vars = "time.from.start",
                                measure.vars=c("scaled.stride.len",
                                               "scaled.stride.height",
                                               "stride.duration"
                                              )
                               ) # ggplot data needs to be tall

  assign(paste0("g",i),
         ggplot(data = df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle(paste0("Participant ",i,": Pre-filtered Resampled Data")) + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  
   assign(paste0("h",i),
         ggplot(data =  median_df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Resampled data post the application of the median filter") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  
  assign(paste0("I",i),
         ggplot(data = cusum_df_transformed,
                aes(x=time.from.start, y=value, group=variable,
                    color=variable,shape=variable)) + 
           geom_line() + theme_bw() + 
           ggtitle("Resampled CUSUM of the data post the application of the median filter") + 
           theme(legend.position="none",
                 #axis.text.x=element_text(angle=90,hjust=1),
                 plot.title = element_text(hjust = 0.5)) +
           facet_wrap(~variable,nrow=3, scales = "free_y")
         )
  
  
  cat('###',paste0("P", i), "{-}",'\n')
  
  print(get(paste0("g",i)))
  cat('\n')
  
  print(get(paste0("h",i)))
  cat('\n')
  
  print(get(paste0("I",i)))
  cat('\n')
  
  cat('\n \n')

}

  cat(paste0('</b> </font> The normalized data can be found at: <a href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/NormMedianFilteredData.RData">NormMedianFilteredData.RData</a> and <a 
href="https://github.com/fmegahed/fatigue-changepoint/blob/master/Data/RGenerated/NormCusumData.RData">NormCusumData.RData</a> </p> </source>')
  )

save(medianfilt_norm,
     file = "./Data/RGenerated/NormMedianFilteredData.RData")

save(cusum_norm,
     file = "./Data/RGenerated/NormCusumData.RData")


```


## Time-Series Clustering
To perform time-series clustering, we can do the following:  
  
  (a) univariate time-series clustering:  
    - using the first PC of the scaled and then median filtered data; or  
    - using the first PC of the scaled, median filtered and cusumed data;  
  
  (b) multivariate time-series clustering:  
    - using the median filtered scaled data;  
    - using the scaled, median filtered and then cusumed data.  
    
We show the results from each analysis in the following subsections.

### Univariate TimeSeries Clustering based on the First PC of the Scaled Sensor Data
```{r TS_Clust, cache=TRUE, fig.align='center', results= 'asis', out.width='65%', message=FALSE, warning=FALSE}
load(file = "./Data/RGenerated/NormMedianFilteredData.RData")
load(file = "./Data/RGenerated/NormCusumData.RData")
# medianfilt_norm, cusum_norm

pca_subjects_loadings_scaled <- list()
pca_subjects_scaled <- data.frame(matrix(nrow=2000, ncol=15))
pc_prop_var_explained_scaled <- vector()

for (i in 1:15) {
  df <- get(paste0("subject",i,"_medianf_norm"))
  pca_dataholder <-  prcomp(df[,2:4], center=TRUE, scale. = T) # Scale <- Standardize Vars
  pca_subjects_loadings_scaled[[i]] <- pca_dataholder$rotation[,1] # Weights for First PC
  # Understanding the % Variation Explained by the First PC
  pc_std <- pca_dataholder$sdev
  pc_var <- pc_std^2
  pc_prop_var_explained_scaled[i] <- round(pc_var[1]/sum(pc_var),3)
  #  Storing the Data for First PC rotated values
  pca_subjects_scaled[,i] <- pca_dataholder$x[,1] # First PC
}

distances <- diss(pca_subjects_scaled, "DTWARP")
dimnames(distances) <- paste0("P",seq(1,15,1))

cat(paste0('<source> <p> Based on the above analysis, the % variation explained by the first principal component of the scaled median filtered data for each participant is equal to: ', paste(round(pc_prop_var_explained_scaled, 2), collapse = ", "), '. To understand how the underlying multivariate time series methods are similar, we applied a heirarchical clustering algorithm based on the distances obtained using the <i> Dynamic Time Warping Method </i>. The resulting dendogram is shown in the figure below. </p> </source>'))

cat("\n")

# distances.matrix <- as.matrix(distances)
# heatmap(distances.matrix)


clusters <- hclust(distances)

plot(clusters, hang=0.1, check = TRUE, 
     axes = TRUE, ann = TRUE, ylab="Height",
     main = "Cluster Dendogram based on the DTWARP distances")


cat("\n")

cat(paste0('Based on the above dendogram, one can see that the best choice for total number of clusters is equal to: 3 or 4. This number was then used to cut of the dendogram tree using the  cutree function in R. We show the resulting cluster membership in the table below.'))

cat("\n")

clusterCut3 <- cutree(clusters, 3)
clusterCut4 <- cutree(clusters, 4)

cat(paste0('The associated cluster assignments for both thresholds are as follows: \n', 'When using k=3, the assignments are as follows: \n'))

kable(t(clusterCut3), row.names = NA, caption = "Cluster Assignment for each Participant | k=3") %>% column_spec(1, bold = T, width = "5em") %>% kable_styling(bootstrap_options = "striped", full_width = F)

cat(paste0(' \n', 'On the other hand, when k=4, the following assignments were made: \n'))

kable(t(clusterCut4), row.names = NA, caption = "Cluster Assignment for each Participant | k=4") %>% column_spec(1, bold = T, width = "5em") %>% kable_styling(bootstrap_options = "striped", full_width = F)

cat("\n")

save(clusterCut3, clusterCut4, pc_prop_var_explained_scaled,
     file = "./Data/RGenerated/ScaledPcaClusters.RData")
```


# Comments
From B and C, we are hoping to develop a general diagnosis/understanding of how fatigue develops (e.g., we start with a ramp up phase, then steady state, then fatigue starting, and then person is totally fatigued).


<!-- # Things to Do Next -->
<!--   (A) Validate the Changepoint results based on the Subjective Ratings of Fatigue.   -->
<!--   (B) Try to come up with a general diagnosis/understanding of how fatigue develops (e.g., we start with a ramp up phase, then steady state, then fatigue starting, and then person is totally fatigued). -->